{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of model for identification of molecular structure from mass spectrum\n",
    "This notebook demonstrates the training and identification of our models.\n",
    "Before running this notebook, you need to run DataFetch.ipynb Preprocess.ipynb train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the depending library.\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import rdkit\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem import AllChem\n",
    "from rdkit import DataStructs\n",
    "\n",
    "from torch_jtnn import *\n",
    "\n",
    "from pcemg.datautil import dataset_load\n",
    "from pcemg.model.ms_encoder import ms_peak_encoder_cnn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load a vocabulary and dataset, and split dataset to train and validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of train dataset:  6586\n",
      "number of validation dataset:  732\n"
     ]
    }
   ],
   "source": [
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "DATASET = \"./massbank.pkl\"\n",
    "\n",
    "with open(VOCAB_FILE,'r') as f:\n",
    "    vocab = [x.strip('\\r\\n') for x in f.readlines()]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "train_vali_rate = 0.9\n",
    "\n",
    "train_dataset,vali_dataset = dataset_load(DATASET,vocab,20,train_vali_rate)\n",
    "with open(\"vali_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(vali_dataset,f)\n",
    "with open(\"train_data.pkl\",\"wb\") as f:\n",
    "    pickle.dump(train_dataset,f)\n",
    "print(\"number of train dataset: \",len(train_dataset))\n",
    "print(\"number of validation dataset: \",len(vali_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/workspace/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=1057, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=135, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=139, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "ms_peak_encoder_cnn(\n",
      "  (embedding): Embedding(1000, 10)\n",
      "  (convSequential): Sequential(\n",
      "    (0): Conv1d(11, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (1): LeakyReLU(negative_slope=0.01)\n",
      "    (2): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "    (4): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (5): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (rnn): GRU(64, 200, num_layers=2, batch_first=True)\n",
      "  (T_mean): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=200, out_features=28, bias=True)\n",
      "  (output): Linear(in_features=56, out_features=56, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_size = 56\n",
    "hidden_size = 100\n",
    "\n",
    "\n",
    "dec_model = JTNNVAE(vocab=vocab,hidden_size=hidden_size,latent_size=latent_size,depthT=20,depthG=5).to('cuda')\n",
    "enc_model = ms_peak_encoder_cnn(train_dataset.max_spectrum_size,output_size=latent_size,\\\n",
    "                               conv1_channel=64,conv2_channel=128,conv_output_channel=256,\\\n",
    "                               kernel1_width=5,kernel2_width=5,conv_output_width=5,\\\n",
    "                                hidden_size=200,embedding_size=10,num_rnn_layers=2,bidirectional=False,dropout_rate=0.2).to('cuda')\n",
    "print(dec_model)\n",
    "print(enc_model)\n",
    "\n",
    "load_model = './vae_model/model.iter-160000'\n",
    "dec_model.load_state_dict(torch.load(load_model,map_location='cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_optimizer = optim.Adam(enc_model.parameters(),lr=1e-03)\n",
    "dec_optimizer = optim.Adam(dec_model.parameters(),lr=1e-03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch :  0\n",
      "(torch.Size([20, 310, 10]),)\n",
      "(torch.Size([20, 310, 1]),)\n",
      "(torch.Size([20, 310, 11]),)\n",
      "(torch.Size([20, 310, 64]),)\n",
      "(torch.Size([20, 310, 64]),)\n",
      "(torch.Size([20, 310, 200]),)\n",
      "(torch.Size([20, 200]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/workspace/lib/python3.6/site-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "/home/kusachi/anaconda3/envs/workspace/lib/python3.6/site-packages/torch/nn/functional.py:1340: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[200] , kl_loss 232.45, Word: 43.22, Topo: 84.18, Assm: 76.36 vali_Word: 47.81, vali_Topo: 83.89, vali_assm: 80.47\n",
      "epoch :  1\n",
      "[400] , kl_loss 264.46, Word: 46.13, Topo: 84.15, Assm: 77.40 vali_Word: 49.89, vali_Topo: 84.27, vali_assm: 80.63\n",
      "[600] , kl_loss 235.35, Word: 48.50, Topo: 84.61, Assm: 78.20 vali_Word: 50.87, vali_Topo: 84.49, vali_assm: 80.16\n",
      "epoch :  2\n",
      "[800] , kl_loss 220.87, Word: 49.17, Topo: 85.06, Assm: 79.30 vali_Word: 46.32, vali_Topo: 84.67, vali_assm: 78.96\n",
      "epoch :  3\n",
      "[1000] , kl_loss 223.07, Word: 49.32, Topo: 84.96, Assm: 79.07 vali_Word: 50.52, vali_Topo: 85.29, vali_assm: 79.79\n",
      "[1200] , kl_loss 226.64, Word: 50.40, Topo: 85.41, Assm: 80.44 vali_Word: 51.29, vali_Topo: 85.22, vali_assm: 79.07\n",
      "epoch :  4\n",
      "[1400] , kl_loss 212.76, Word: 49.77, Topo: 85.12, Assm: 78.32 vali_Word: 52.11, vali_Topo: 85.73, vali_assm: 78.32\n",
      "[1600] , kl_loss 216.88, Word: 50.85, Topo: 85.71, Assm: 79.80 vali_Word: 51.71, vali_Topo: 85.58, vali_assm: 79.79\n",
      "epoch :  5\n",
      "[1800] , kl_loss 218.70, Word: 50.54, Topo: 85.49, Assm: 78.79 vali_Word: 50.39, vali_Topo: 85.47, vali_assm: 79.38\n",
      "epoch :  6\n",
      "[2000] , kl_loss 215.52, Word: 51.15, Topo: 85.75, Assm: 79.95 vali_Word: 51.87, vali_Topo: 86.26, vali_assm: 80.30\n",
      "[2200] , kl_loss 213.98, Word: 51.31, Topo: 85.63, Assm: 78.94 vali_Word: 50.75, vali_Topo: 86.10, vali_assm: 79.95\n",
      "epoch :  7\n",
      "[2400] , kl_loss 208.39, Word: 52.29, Topo: 85.99, Assm: 80.09 vali_Word: 52.92, vali_Topo: 86.16, vali_assm: 81.05\n",
      "[2600] , kl_loss 207.48, Word: 52.11, Topo: 86.20, Assm: 79.81 vali_Word: 52.09, vali_Topo: 86.04, vali_assm: 81.08\n",
      "epoch :  8\n",
      "[2800] , kl_loss 213.37, Word: 51.85, Topo: 86.29, Assm: 80.05 vali_Word: 52.34, vali_Topo: 86.46, vali_assm: 79.80\n",
      "epoch :  9\n",
      "[3000] , kl_loss 213.02, Word: 51.90, Topo: 86.13, Assm: 79.96 vali_Word: 52.04, vali_Topo: 86.67, vali_assm: 80.94\n",
      "[3200] , kl_loss 217.47, Word: 52.53, Topo: 86.32, Assm: 79.70 vali_Word: 51.31, vali_Topo: 86.17, vali_assm: 78.55\n",
      "epoch :  10\n",
      "[3400] , kl_loss 219.12, Word: 52.47, Topo: 86.33, Assm: 79.97 vali_Word: 52.83, vali_Topo: 86.70, vali_assm: 79.53\n",
      "[3600] , kl_loss 217.06, Word: 52.81, Topo: 86.37, Assm: 80.43 vali_Word: 53.82, vali_Topo: 86.35, vali_assm: 79.20\n",
      "epoch :  11\n",
      "[3800] , kl_loss 216.64, Word: 53.56, Topo: 86.79, Assm: 80.97 vali_Word: 53.78, vali_Topo: 86.88, vali_assm: 80.03\n",
      "epoch :  12\n",
      "[4000] , kl_loss 223.70, Word: 53.22, Topo: 86.86, Assm: 79.82 vali_Word: 52.84, vali_Topo: 86.90, vali_assm: 81.49\n",
      "[4200] , kl_loss 225.16, Word: 52.57, Topo: 86.53, Assm: 80.10 vali_Word: 53.23, vali_Topo: 86.79, vali_assm: 80.61\n",
      "epoch :  13\n",
      "[4400] , kl_loss 230.68, Word: 53.47, Topo: 86.57, Assm: 81.39 vali_Word: 52.95, vali_Topo: 86.83, vali_assm: 79.90\n",
      "[4600] , kl_loss 231.42, Word: 53.61, Topo: 86.88, Assm: 79.95 vali_Word: 54.76, vali_Topo: 86.84, vali_assm: 80.83\n",
      "epoch :  14\n",
      "[4800] , kl_loss 234.33, Word: 53.85, Topo: 87.11, Assm: 80.50 vali_Word: 51.67, vali_Topo: 87.07, vali_assm: 82.07\n",
      "epoch :  15\n",
      "[5000] , kl_loss 235.26, Word: 53.81, Topo: 87.07, Assm: 80.31 vali_Word: 53.56, vali_Topo: 87.03, vali_assm: 82.05\n",
      "[5200] , kl_loss 238.81, Word: 54.34, Topo: 87.00, Assm: 81.49 vali_Word: 52.44, vali_Topo: 87.05, vali_assm: 80.29\n",
      "epoch :  16\n",
      "[5400] , kl_loss 238.10, Word: 55.15, Topo: 87.32, Assm: 81.28 vali_Word: 55.96, vali_Topo: 87.00, vali_assm: 82.34\n",
      "epoch :  17\n",
      "[5600] , kl_loss 238.12, Word: 54.45, Topo: 87.37, Assm: 80.52 vali_Word: 53.33, vali_Topo: 87.04, vali_assm: 81.28\n",
      "[5800] , kl_loss 234.83, Word: 54.89, Topo: 87.48, Assm: 81.22 vali_Word: 53.87, vali_Topo: 87.27, vali_assm: 80.68\n",
      "epoch :  18\n",
      "[6000] , kl_loss 236.27, Word: 55.23, Topo: 87.31, Assm: 80.44 vali_Word: 53.83, vali_Topo: 87.15, vali_assm: 80.95\n",
      "[6200] , kl_loss 241.52, Word: 55.02, Topo: 87.64, Assm: 80.66 vali_Word: 54.67, vali_Topo: 87.10, vali_assm: 81.43\n",
      "epoch :  19\n",
      "[6400] , kl_loss 242.84, Word: 55.34, Topo: 87.84, Assm: 81.43 vali_Word: 54.65, vali_Topo: 87.53, vali_assm: 81.52\n",
      "epoch :  20\n",
      "[6600] , kl_loss 240.86, Word: 55.43, Topo: 87.52, Assm: 80.87 vali_Word: 56.16, vali_Topo: 87.57, vali_assm: 81.86\n",
      "[6800] , kl_loss 245.91, Word: 56.39, Topo: 87.98, Assm: 81.36 vali_Word: 55.11, vali_Topo: 87.73, vali_assm: 80.43\n",
      "epoch :  21\n",
      "[7000] , kl_loss 249.38, Word: 56.06, Topo: 87.80, Assm: 80.71 vali_Word: 54.60, vali_Topo: 87.65, vali_assm: 82.64\n",
      "[7200] , kl_loss 248.47, Word: 56.05, Topo: 87.92, Assm: 80.94 vali_Word: 54.87, vali_Topo: 87.71, vali_assm: 81.35\n",
      "epoch :  22\n",
      "[7400] , kl_loss 250.41, Word: 56.51, Topo: 88.03, Assm: 81.46 vali_Word: 55.98, vali_Topo: 87.86, vali_assm: 81.80\n",
      "epoch :  23\n",
      "[7600] , kl_loss 257.35, Word: 56.44, Topo: 87.84, Assm: 81.12 vali_Word: 55.35, vali_Topo: 86.96, vali_assm: 81.03\n",
      "[7800] , kl_loss 265.14, Word: 55.96, Topo: 87.85, Assm: 81.32 vali_Word: 54.78, vali_Topo: 87.67, vali_assm: 82.37\n",
      "epoch :  24\n",
      "[8000] , kl_loss 264.43, Word: 56.97, Topo: 88.20, Assm: 82.62 vali_Word: 55.54, vali_Topo: 87.23, vali_assm: 81.51\n",
      "[8200] , kl_loss 255.78, Word: 56.65, Topo: 88.14, Assm: 80.58 vali_Word: 55.97, vali_Topo: 87.65, vali_assm: 81.19\n",
      "epoch :  25\n",
      "[8400] , kl_loss 257.23, Word: 57.62, Topo: 88.46, Assm: 81.94 vali_Word: 56.43, vali_Topo: 87.91, vali_assm: 81.66\n",
      "epoch :  26\n",
      "[8600] , kl_loss 258.86, Word: 56.97, Topo: 88.34, Assm: 81.44 vali_Word: 56.31, vali_Topo: 88.00, vali_assm: 81.69\n",
      "[8800] , kl_loss 261.35, Word: 57.77, Topo: 88.52, Assm: 81.23 vali_Word: 57.19, vali_Topo: 87.61, vali_assm: 80.02\n",
      "epoch :  27\n",
      "[9000] , kl_loss 266.01, Word: 57.73, Topo: 88.54, Assm: 82.05 vali_Word: 55.45, vali_Topo: 87.74, vali_assm: 82.61\n",
      "[9200] , kl_loss 269.01, Word: 58.48, Topo: 88.55, Assm: 81.77 vali_Word: 56.75, vali_Topo: 87.92, vali_assm: 81.24\n",
      "epoch :  28\n",
      "[9400] , kl_loss 268.06, Word: 58.70, Topo: 88.97, Assm: 82.24 vali_Word: 56.56, vali_Topo: 88.08, vali_assm: 82.43\n",
      "epoch :  29\n",
      "[9600] , kl_loss 268.15, Word: 58.49, Topo: 88.54, Assm: 82.05 vali_Word: 56.93, vali_Topo: 87.77, vali_assm: 81.96\n",
      "[9800] , kl_loss 267.02, Word: 58.69, Topo: 88.82, Assm: 82.07 vali_Word: 57.33, vali_Topo: 87.79, vali_assm: 82.32\n",
      "epoch :  30\n",
      "[10000] , kl_loss 263.55, Word: 61.71, Topo: 91.16, Assm: 83.14 vali_Word: 65.35, vali_Topo: 92.29, vali_assm: 85.43\n",
      "epoch :  31\n",
      "[10200] , kl_loss 253.82, Word: 67.18, Topo: 93.66, Assm: 85.37 vali_Word: 67.09, vali_Topo: 93.73, vali_assm: 87.19\n",
      "[10400] , kl_loss 246.84, Word: 69.41, Topo: 94.75, Assm: 88.60 vali_Word: 68.22, vali_Topo: 93.96, vali_assm: 86.57\n",
      "epoch :  32\n",
      "[10600] , kl_loss 244.68, Word: 70.50, Topo: 95.00, Assm: 89.74 vali_Word: 68.59, vali_Topo: 94.57, vali_assm: 88.22\n",
      "[10800] , kl_loss 246.99, Word: 70.90, Topo: 95.30, Assm: 89.80 vali_Word: 69.09, vali_Topo: 94.03, vali_assm: 88.29\n",
      "epoch :  33\n",
      "[11000] , kl_loss 246.17, Word: 71.78, Topo: 95.65, Assm: 90.89 vali_Word: 68.43, vali_Topo: 94.51, vali_assm: 88.67\n",
      "epoch :  34\n",
      "[11200] , kl_loss 245.92, Word: 72.39, Topo: 95.70, Assm: 91.09 vali_Word: 70.23, vali_Topo: 94.70, vali_assm: 89.45\n",
      "[11400] , kl_loss 246.27, Word: 72.59, Topo: 95.90, Assm: 91.40 vali_Word: 69.76, vali_Topo: 94.86, vali_assm: 89.11\n",
      "epoch :  35\n",
      "[11600] , kl_loss 251.41, Word: 72.93, Topo: 96.05, Assm: 92.10 vali_Word: 69.69, vali_Topo: 94.84, vali_assm: 88.74\n",
      "[11800] , kl_loss 249.06, Word: 73.56, Topo: 96.25, Assm: 91.82 vali_Word: 70.39, vali_Topo: 94.76, vali_assm: 89.98\n",
      "epoch :  36\n",
      "[12000] , kl_loss 252.58, Word: 73.63, Topo: 96.39, Assm: 92.70 vali_Word: 70.96, vali_Topo: 94.97, vali_assm: 88.86\n",
      "epoch :  37\n",
      "[12200] , kl_loss 249.78, Word: 74.21, Topo: 96.25, Assm: 92.44 vali_Word: 71.38, vali_Topo: 95.19, vali_assm: 90.45\n",
      "[12400] , kl_loss 250.58, Word: 74.64, Topo: 96.49, Assm: 93.18 vali_Word: 70.87, vali_Topo: 95.05, vali_assm: 90.29\n",
      "epoch :  38\n",
      "[12600] , kl_loss 248.10, Word: 74.45, Topo: 96.48, Assm: 93.14 vali_Word: 71.45, vali_Topo: 94.88, vali_assm: 89.49\n",
      "[12800] , kl_loss 252.33, Word: 75.02, Topo: 96.59, Assm: 92.88 vali_Word: 70.95, vali_Topo: 94.98, vali_assm: 89.41\n",
      "epoch :  39\n",
      "[13000] , kl_loss 249.79, Word: 75.60, Topo: 96.85, Assm: 93.82 vali_Word: 70.68, vali_Topo: 95.21, vali_assm: 89.69\n",
      "epoch :  40\n",
      "[13200] , kl_loss 249.19, Word: 75.16, Topo: 96.76, Assm: 92.95 vali_Word: 71.13, vali_Topo: 95.26, vali_assm: 89.59\n",
      "[13400] , kl_loss 249.03, Word: 75.74, Topo: 96.96, Assm: 93.62 vali_Word: 71.78, vali_Topo: 95.20, vali_assm: 90.52\n",
      "epoch :  41\n",
      "[13600] , kl_loss 252.09, Word: 75.66, Topo: 96.79, Assm: 93.72 vali_Word: 71.76, vali_Topo: 95.22, vali_assm: 89.94\n",
      "[13800] , kl_loss 251.26, Word: 76.33, Topo: 96.97, Assm: 93.96 vali_Word: 72.34, vali_Topo: 95.66, vali_assm: 90.97\n",
      "epoch :  42\n",
      "[14000] , kl_loss 255.23, Word: 76.27, Topo: 97.26, Assm: 93.86 vali_Word: 71.79, vali_Topo: 95.49, vali_assm: 90.41\n",
      "epoch :  43\n",
      "[14200] , kl_loss 256.72, Word: 77.01, Topo: 97.04, Assm: 94.13 vali_Word: 71.58, vali_Topo: 95.41, vali_assm: 89.21\n",
      "[14400] , kl_loss 260.62, Word: 76.98, Topo: 97.27, Assm: 94.28 vali_Word: 71.93, vali_Topo: 95.43, vali_assm: 89.58\n",
      "epoch :  44\n",
      "[14600] , kl_loss 258.26, Word: 76.79, Topo: 97.24, Assm: 94.09 vali_Word: 72.59, vali_Topo: 95.33, vali_assm: 90.52\n",
      "[14800] , kl_loss 259.04, Word: 77.25, Topo: 97.33, Assm: 94.15 vali_Word: 72.12, vali_Topo: 95.50, vali_assm: 90.21\n",
      "epoch :  45\n",
      "[15000] , kl_loss 263.19, Word: 77.84, Topo: 97.45, Assm: 94.88 vali_Word: 71.63, vali_Topo: 95.48, vali_assm: 89.62\n",
      "epoch :  46\n",
      "[15200] , kl_loss 262.64, Word: 77.83, Topo: 97.42, Assm: 94.74 vali_Word: 72.39, vali_Topo: 95.47, vali_assm: 88.63\n",
      "[15400] , kl_loss 260.85, Word: 78.45, Topo: 97.57, Assm: 94.44 vali_Word: 72.13, vali_Topo: 95.52, vali_assm: 89.83\n",
      "epoch :  47\n",
      "[15600] , kl_loss 263.68, Word: 78.38, Topo: 97.49, Assm: 95.34 vali_Word: 71.94, vali_Topo: 95.50, vali_assm: 90.83\n",
      "epoch :  48\n",
      "[15800] , kl_loss 266.59, Word: 78.60, Topo: 97.62, Assm: 94.41 vali_Word: 72.39, vali_Topo: 95.55, vali_assm: 89.95\n",
      "[16000] , kl_loss 266.19, Word: 78.81, Topo: 97.70, Assm: 95.21 vali_Word: 72.39, vali_Topo: 95.63, vali_assm: 90.28\n",
      "epoch :  49\n",
      "[16200] , kl_loss 266.87, Word: 78.97, Topo: 97.64, Assm: 94.88 vali_Word: 72.13, vali_Topo: 95.58, vali_assm: 88.71\n",
      "[16400] , kl_loss 271.01, Word: 79.20, Topo: 97.67, Assm: 94.69 vali_Word: 72.24, vali_Topo: 95.49, vali_assm: 89.57\n",
      "epoch :  50\n",
      "[16600] , kl_loss 213.76, Word: 79.06, Topo: 97.83, Assm: 95.14 vali_Word: 73.11, vali_Topo: 95.81, vali_assm: 90.02\n",
      "epoch :  51\n",
      "[16800] , kl_loss 141.54, Word: 78.97, Topo: 97.77, Assm: 94.13 vali_Word: 72.73, vali_Topo: 95.62, vali_assm: 89.32\n",
      "[17000] , kl_loss 133.91, Word: 79.66, Topo: 97.92, Assm: 94.68 vali_Word: 73.44, vali_Topo: 95.60, vali_assm: 89.17\n",
      "epoch :  52\n",
      "[17200] , kl_loss 129.97, Word: 79.74, Topo: 97.80, Assm: 95.13 vali_Word: 72.74, vali_Topo: 95.95, vali_assm: 89.94\n",
      "[17400] , kl_loss 125.54, Word: 79.87, Topo: 97.76, Assm: 95.04 vali_Word: 72.84, vali_Topo: 95.51, vali_assm: 90.55\n",
      "epoch :  53\n",
      "[17600] , kl_loss 123.85, Word: 80.20, Topo: 97.99, Assm: 95.29 vali_Word: 73.09, vali_Topo: 95.91, vali_assm: 89.42\n",
      "epoch :  54\n",
      "[17800] , kl_loss 121.40, Word: 79.81, Topo: 97.99, Assm: 94.69 vali_Word: 72.82, vali_Topo: 95.81, vali_assm: 89.58\n",
      "[18000] , kl_loss 122.23, Word: 80.70, Topo: 98.09, Assm: 95.42 vali_Word: 73.57, vali_Topo: 95.96, vali_assm: 89.08\n",
      "epoch :  55\n",
      "[18200] , kl_loss 120.24, Word: 80.65, Topo: 98.10, Assm: 94.90 vali_Word: 73.10, vali_Topo: 96.03, vali_assm: 90.58\n",
      "[18400] , kl_loss 120.88, Word: 80.48, Topo: 98.14, Assm: 95.22 vali_Word: 73.45, vali_Topo: 95.58, vali_assm: 91.19\n",
      "epoch :  56\n",
      "[18600] , kl_loss 119.21, Word: 81.15, Topo: 98.18, Assm: 95.45 vali_Word: 73.01, vali_Topo: 95.72, vali_assm: 89.27\n",
      "epoch :  57\n",
      "[18800] , kl_loss 118.20, Word: 81.10, Topo: 98.13, Assm: 95.50 vali_Word: 73.27, vali_Topo: 95.95, vali_assm: 89.64\n",
      "[19000] , kl_loss 120.72, Word: 81.46, Topo: 98.17, Assm: 95.85 vali_Word: 73.01, vali_Topo: 95.54, vali_assm: 89.90\n",
      "epoch :  58\n",
      "[19200] , kl_loss 119.90, Word: 81.25, Topo: 98.23, Assm: 95.38 vali_Word: 73.30, vali_Topo: 96.03, vali_assm: 90.27\n",
      "[19400] , kl_loss 118.50, Word: 81.29, Topo: 98.17, Assm: 95.13 vali_Word: 72.46, vali_Topo: 95.88, vali_assm: 89.52\n",
      "epoch :  59\n",
      "[19600] , kl_loss 117.02, Word: 81.95, Topo: 98.19, Assm: 95.74 vali_Word: 73.16, vali_Topo: 95.75, vali_assm: 89.80\n",
      "epoch :  60\n",
      "[19800] , kl_loss 115.73, Word: 82.01, Topo: 98.19, Assm: 95.25 vali_Word: 73.90, vali_Topo: 95.79, vali_assm: 90.37\n",
      "[20000] , kl_loss 103.69, Word: 81.78, Topo: 98.24, Assm: 95.25 vali_Word: 73.83, vali_Topo: 95.95, vali_assm: 91.02\n",
      "epoch :  61\n",
      "[20200] , kl_loss 101.58, Word: 82.02, Topo: 98.39, Assm: 95.84 vali_Word: 73.15, vali_Topo: 95.90, vali_assm: 90.05\n",
      "epoch :  62\n",
      "[20400] , kl_loss 99.15, Word: 82.13, Topo: 98.30, Assm: 95.89 vali_Word: 73.20, vali_Topo: 96.11, vali_assm: 89.79\n",
      "[20600] , kl_loss 99.34, Word: 82.31, Topo: 98.46, Assm: 96.04 vali_Word: 73.46, vali_Topo: 95.72, vali_assm: 89.52\n",
      "epoch :  63\n",
      "[20800] , kl_loss 96.38, Word: 82.40, Topo: 98.33, Assm: 95.89 vali_Word: 72.42, vali_Topo: 95.70, vali_assm: 89.62\n",
      "[21000] , kl_loss 97.10, Word: 82.14, Topo: 98.39, Assm: 95.66 vali_Word: 73.64, vali_Topo: 95.65, vali_assm: 89.37\n",
      "epoch :  64\n",
      "[21200] , kl_loss 95.07, Word: 83.00, Topo: 98.38, Assm: 95.71 vali_Word: 73.45, vali_Topo: 95.94, vali_assm: 90.22\n",
      "epoch :  65\n",
      "[21400] , kl_loss 96.24, Word: 82.40, Topo: 98.40, Assm: 95.58 vali_Word: 72.61, vali_Topo: 95.82, vali_assm: 89.51\n",
      "[21600] , kl_loss 95.95, Word: 83.29, Topo: 98.50, Assm: 95.68 vali_Word: 73.79, vali_Topo: 95.89, vali_assm: 89.32\n",
      "epoch :  66\n",
      "[21800] , kl_loss 94.28, Word: 83.23, Topo: 98.43, Assm: 95.96 vali_Word: 73.46, vali_Topo: 95.86, vali_assm: 89.57\n",
      "[22000] , kl_loss 94.20, Word: 82.82, Topo: 98.38, Assm: 95.71 vali_Word: 72.67, vali_Topo: 96.08, vali_assm: 89.30\n",
      "epoch :  67\n",
      "[22200] , kl_loss 95.02, Word: 83.61, Topo: 98.55, Assm: 95.42 vali_Word: 73.02, vali_Topo: 95.96, vali_assm: 89.73\n",
      "epoch :  68\n",
      "[22400] , kl_loss 95.39, Word: 83.24, Topo: 98.51, Assm: 95.68 vali_Word: 73.06, vali_Topo: 95.93, vali_assm: 88.18\n",
      "[22600] , kl_loss 94.13, Word: 83.85, Topo: 98.55, Assm: 95.88 vali_Word: 73.28, vali_Topo: 95.99, vali_assm: 89.85\n",
      "epoch :  69\n",
      "[22800] , kl_loss 94.03, Word: 83.75, Topo: 98.58, Assm: 96.06 vali_Word: 73.61, vali_Topo: 96.05, vali_assm: 89.39\n",
      "[23000] , kl_loss 94.23, Word: 83.58, Topo: 98.42, Assm: 95.95 vali_Word: 73.41, vali_Topo: 96.07, vali_assm: 89.97\n",
      "epoch :  70\n",
      "[23200] , kl_loss 88.04, Word: 84.06, Topo: 98.58, Assm: 95.88 vali_Word: 73.87, vali_Topo: 95.95, vali_assm: 91.01\n",
      "epoch :  71\n",
      "[23400] , kl_loss 84.32, Word: 83.72, Topo: 98.63, Assm: 95.52 vali_Word: 73.19, vali_Topo: 96.14, vali_assm: 90.67\n",
      "[23600] , kl_loss 83.87, Word: 84.44, Topo: 98.63, Assm: 96.16 vali_Word: 73.13, vali_Topo: 95.84, vali_assm: 89.96\n",
      "epoch :  72\n",
      "[23800] , kl_loss 82.67, Word: 84.39, Topo: 98.63, Assm: 96.12 vali_Word: 74.07, vali_Topo: 96.00, vali_assm: 90.78\n",
      "[24000] , kl_loss 82.21, Word: 84.10, Topo: 98.55, Assm: 96.11 vali_Word: 74.42, vali_Topo: 96.01, vali_assm: 89.74\n",
      "epoch :  73\n",
      "[24200] , kl_loss 82.00, Word: 84.35, Topo: 98.65, Assm: 95.83 vali_Word: 72.84, vali_Topo: 95.75, vali_assm: 90.44\n",
      "epoch :  74\n",
      "[24400] , kl_loss 81.05, Word: 84.36, Topo: 98.64, Assm: 96.04 vali_Word: 73.12, vali_Topo: 95.92, vali_assm: 89.83\n",
      "[24600] , kl_loss 79.59, Word: 84.62, Topo: 98.61, Assm: 95.89 vali_Word: 73.84, vali_Topo: 96.03, vali_assm: 89.80\n",
      "epoch :  75\n",
      "[24800] , kl_loss 80.31, Word: 84.44, Topo: 98.67, Assm: 96.09 vali_Word: 72.28, vali_Topo: 95.76, vali_assm: 89.73\n",
      "[25000] , kl_loss 79.91, Word: 84.92, Topo: 98.72, Assm: 96.41 vali_Word: 72.83, vali_Topo: 95.87, vali_assm: 90.30\n",
      "epoch :  76\n",
      "[25200] , kl_loss 78.84, Word: 85.17, Topo: 98.74, Assm: 96.16 vali_Word: 73.92, vali_Topo: 95.74, vali_assm: 90.37\n",
      "epoch :  77\n",
      "[25400] , kl_loss 80.29, Word: 85.02, Topo: 98.71, Assm: 96.76 vali_Word: 73.89, vali_Topo: 96.02, vali_assm: 90.34\n",
      "[25600] , kl_loss 80.05, Word: 84.75, Topo: 98.65, Assm: 96.09 vali_Word: 73.47, vali_Topo: 95.92, vali_assm: 89.73\n",
      "epoch :  78\n",
      "[25800] , kl_loss 79.52, Word: 85.29, Topo: 98.66, Assm: 96.64 vali_Word: 73.27, vali_Topo: 95.80, vali_assm: 89.65\n",
      "epoch :  79\n",
      "[26000] , kl_loss 79.03, Word: 85.55, Topo: 98.75, Assm: 96.01 vali_Word: 73.68, vali_Topo: 95.94, vali_assm: 90.35\n",
      "[26200] , kl_loss 77.26, Word: 85.38, Topo: 98.78, Assm: 96.40 vali_Word: 73.17, vali_Topo: 95.95, vali_assm: 89.55\n",
      "epoch :  80\n",
      "[26400] , kl_loss 77.19, Word: 85.59, Topo: 98.68, Assm: 96.22 vali_Word: 73.42, vali_Topo: 95.92, vali_assm: 89.36\n",
      "[26600] , kl_loss 73.51, Word: 85.14, Topo: 98.74, Assm: 95.95 vali_Word: 73.44, vali_Topo: 96.08, vali_assm: 90.86\n",
      "epoch :  81\n",
      "[26800] , kl_loss 72.63, Word: 85.67, Topo: 98.80, Assm: 95.91 vali_Word: 73.65, vali_Topo: 96.04, vali_assm: 89.78\n",
      "epoch :  82\n",
      "[27000] , kl_loss 71.23, Word: 85.75, Topo: 98.73, Assm: 95.69 vali_Word: 73.93, vali_Topo: 95.93, vali_assm: 90.82\n",
      "[27200] , kl_loss 70.48, Word: 86.11, Topo: 98.78, Assm: 96.43 vali_Word: 72.64, vali_Topo: 95.96, vali_assm: 90.53\n",
      "epoch :  83\n",
      "[27400] , kl_loss 71.02, Word: 85.82, Topo: 98.77, Assm: 96.56 vali_Word: 73.95, vali_Topo: 96.06, vali_assm: 90.44\n",
      "[27600] , kl_loss 70.44, Word: 85.46, Topo: 98.76, Assm: 96.47 vali_Word: 73.35, vali_Topo: 95.83, vali_assm: 90.70\n",
      "epoch :  84\n",
      "[27800] , kl_loss 69.72, Word: 86.34, Topo: 98.90, Assm: 96.89 vali_Word: 73.03, vali_Topo: 96.03, vali_assm: 89.67\n",
      "epoch :  85\n",
      "[28000] , kl_loss 69.54, Word: 85.78, Topo: 98.75, Assm: 96.17 vali_Word: 73.11, vali_Topo: 95.81, vali_assm: 90.65\n",
      "[28200] , kl_loss 68.97, Word: 86.26, Topo: 98.83, Assm: 96.56 vali_Word: 73.33, vali_Topo: 95.97, vali_assm: 89.47\n",
      "epoch :  86\n",
      "[28400] , kl_loss 68.45, Word: 86.08, Topo: 98.88, Assm: 96.58 vali_Word: 73.83, vali_Topo: 96.03, vali_assm: 90.37\n",
      "[28600] , kl_loss 68.47, Word: 85.81, Topo: 98.81, Assm: 96.28 vali_Word: 73.97, vali_Topo: 96.05, vali_assm: 90.24\n",
      "epoch :  87\n",
      "[28800] , kl_loss 68.42, Word: 86.59, Topo: 98.93, Assm: 96.21 vali_Word: 73.90, vali_Topo: 95.77, vali_assm: 89.81\n",
      "epoch :  88\n",
      "[29000] , kl_loss 68.18, Word: 86.29, Topo: 98.81, Assm: 96.69 vali_Word: 73.45, vali_Topo: 95.93, vali_assm: 89.53\n",
      "[29200] , kl_loss 67.49, Word: 86.48, Topo: 98.82, Assm: 96.44 vali_Word: 74.00, vali_Topo: 95.84, vali_assm: 89.45\n",
      "epoch :  89\n",
      "[29400] , kl_loss 67.55, Word: 86.77, Topo: 98.86, Assm: 96.27 vali_Word: 73.63, vali_Topo: 95.99, vali_assm: 89.69\n",
      "[29600] , kl_loss 67.55, Word: 86.48, Topo: 98.81, Assm: 96.73 vali_Word: 73.31, vali_Topo: 96.22, vali_assm: 90.02\n",
      "epoch :  90\n",
      "[29800] , kl_loss 66.02, Word: 86.84, Topo: 98.94, Assm: 96.70 vali_Word: 72.77, vali_Topo: 96.13, vali_assm: 90.43\n",
      "epoch :  91\n",
      "[30000] , kl_loss 63.53, Word: 86.80, Topo: 98.95, Assm: 96.21 vali_Word: 73.93, vali_Topo: 95.94, vali_assm: 89.66\n",
      "[30200] , kl_loss 62.47, Word: 86.94, Topo: 98.89, Assm: 95.83 vali_Word: 73.48, vali_Topo: 95.97, vali_assm: 90.39\n",
      "epoch :  92\n",
      "[30400] , kl_loss 62.53, Word: 87.23, Topo: 98.91, Assm: 96.48 vali_Word: 73.07, vali_Topo: 95.86, vali_assm: 90.84\n",
      "epoch :  93\n",
      "[30600] , kl_loss 61.93, Word: 86.74, Topo: 98.87, Assm: 96.07 vali_Word: 74.21, vali_Topo: 96.02, vali_assm: 91.25\n",
      "[30800] , kl_loss 60.79, Word: 86.98, Topo: 98.94, Assm: 96.51 vali_Word: 73.47, vali_Topo: 95.95, vali_assm: 90.04\n",
      "epoch :  94\n",
      "[31000] , kl_loss 61.63, Word: 87.04, Topo: 98.93, Assm: 96.60 vali_Word: 72.95, vali_Topo: 96.06, vali_assm: 91.20\n",
      "[31200] , kl_loss 61.27, Word: 87.00, Topo: 98.91, Assm: 96.43 vali_Word: 73.28, vali_Topo: 95.96, vali_assm: 89.89\n",
      "epoch :  95\n",
      "[31400] , kl_loss 61.10, Word: 87.36, Topo: 98.89, Assm: 96.62 vali_Word: 72.87, vali_Topo: 96.08, vali_assm: 89.52\n",
      "epoch :  96\n",
      "[31600] , kl_loss 60.88, Word: 87.27, Topo: 98.97, Assm: 96.42 vali_Word: 73.37, vali_Topo: 95.60, vali_assm: 89.29\n",
      "[31800] , kl_loss 60.81, Word: 87.47, Topo: 99.03, Assm: 96.91 vali_Word: 74.30, vali_Topo: 96.07, vali_assm: 90.20\n",
      "epoch :  97\n",
      "[32000] , kl_loss 60.87, Word: 87.53, Topo: 98.95, Assm: 96.49 vali_Word: 73.26, vali_Topo: 96.00, vali_assm: 89.87\n",
      "[32200] , kl_loss 60.73, Word: 87.08, Topo: 98.85, Assm: 96.63 vali_Word: 73.07, vali_Topo: 95.82, vali_assm: 90.22\n",
      "epoch :  98\n",
      "[32400] , kl_loss 61.45, Word: 87.84, Topo: 98.98, Assm: 96.57 vali_Word: 73.74, vali_Topo: 96.07, vali_assm: 90.11\n",
      "epoch :  99\n",
      "[32600] , kl_loss 60.04, Word: 87.16, Topo: 98.88, Assm: 96.64 vali_Word: 73.82, vali_Topo: 96.05, vali_assm: 90.84\n",
      "[32800] , kl_loss 60.51, Word: 87.88, Topo: 99.01, Assm: 96.73 vali_Word: 73.69, vali_Topo: 95.82, vali_assm: 89.64\n",
      "epoch :  100\n",
      "[33000] , kl_loss 59.13, Word: 87.49, Topo: 98.98, Assm: 97.02 vali_Word: 73.78, vali_Topo: 96.06, vali_assm: 90.66\n",
      "[33200] , kl_loss 57.95, Word: 87.55, Topo: 98.99, Assm: 96.75 vali_Word: 73.83, vali_Topo: 95.94, vali_assm: 90.18\n",
      "epoch :  101\n",
      "[33400] , kl_loss 57.68, Word: 87.98, Topo: 99.01, Assm: 96.73 vali_Word: 74.23, vali_Topo: 96.24, vali_assm: 89.89\n",
      "epoch :  102\n",
      "[33600] , kl_loss 57.29, Word: 87.93, Topo: 98.96, Assm: 96.68 vali_Word: 73.29, vali_Topo: 95.90, vali_assm: 91.12\n",
      "[33800] , kl_loss 56.76, Word: 88.09, Topo: 99.02, Assm: 96.98 vali_Word: 73.58, vali_Topo: 96.05, vali_assm: 89.92\n",
      "epoch :  103\n",
      "[34000] , kl_loss 57.00, Word: 88.19, Topo: 99.11, Assm: 96.66 vali_Word: 73.82, vali_Topo: 96.02, vali_assm: 90.44\n",
      "[34200] , kl_loss 56.11, Word: 87.70, Topo: 98.95, Assm: 96.84 vali_Word: 73.66, vali_Topo: 95.90, vali_assm: 90.03\n",
      "epoch :  104\n",
      "[34400] , kl_loss 55.89, Word: 88.35, Topo: 99.03, Assm: 97.12 vali_Word: 73.44, vali_Topo: 95.87, vali_assm: 90.03\n",
      "epoch :  105\n",
      "[34600] , kl_loss 55.81, Word: 87.89, Topo: 98.96, Assm: 96.57 vali_Word: 74.07, vali_Topo: 95.65, vali_assm: 90.18\n",
      "[34800] , kl_loss 55.55, Word: 88.32, Topo: 99.05, Assm: 96.63 vali_Word: 73.71, vali_Topo: 96.00, vali_assm: 90.17\n",
      "epoch :  106\n",
      "[35000] , kl_loss 54.46, Word: 88.40, Topo: 99.08, Assm: 97.03 vali_Word: 74.15, vali_Topo: 96.04, vali_assm: 89.52\n",
      "[35200] , kl_loss 54.93, Word: 88.36, Topo: 98.99, Assm: 96.56 vali_Word: 74.31, vali_Topo: 95.89, vali_assm: 89.68\n",
      "epoch :  107\n",
      "[35400] , kl_loss 54.76, Word: 88.62, Topo: 99.06, Assm: 96.98 vali_Word: 73.45, vali_Topo: 95.97, vali_assm: 89.53\n",
      "epoch :  108\n",
      "[35600] , kl_loss 55.14, Word: 88.08, Topo: 99.01, Assm: 96.15 vali_Word: 74.47, vali_Topo: 96.10, vali_assm: 89.05\n",
      "[35800] , kl_loss 54.81, Word: 88.32, Topo: 98.91, Assm: 96.50 vali_Word: 73.42, vali_Topo: 96.09, vali_assm: 90.00\n",
      "epoch :  109\n",
      "[36000] , kl_loss 55.13, Word: 88.25, Topo: 99.05, Assm: 97.29 vali_Word: 74.54, vali_Topo: 96.02, vali_assm: 90.40\n",
      "epoch :  110\n",
      "[36200] , kl_loss 54.83, Word: 87.58, Topo: 98.90, Assm: 96.74 vali_Word: 73.37, vali_Topo: 95.94, vali_assm: 91.10\n",
      "[36400] , kl_loss 53.61, Word: 88.07, Topo: 99.03, Assm: 96.78 vali_Word: 74.20, vali_Topo: 96.21, vali_assm: 90.50\n",
      "epoch :  111\n",
      "[36600] , kl_loss 53.23, Word: 88.26, Topo: 99.04, Assm: 96.94 vali_Word: 73.36, vali_Topo: 95.90, vali_assm: 89.69\n",
      "[36800] , kl_loss 52.25, Word: 88.48, Topo: 99.06, Assm: 96.74 vali_Word: 73.06, vali_Topo: 95.97, vali_assm: 90.51\n",
      "epoch :  112\n",
      "[37000] , kl_loss 52.11, Word: 88.87, Topo: 99.10, Assm: 96.69 vali_Word: 73.27, vali_Topo: 95.87, vali_assm: 89.31\n",
      "epoch :  113\n",
      "[37200] , kl_loss 52.28, Word: 88.61, Topo: 99.13, Assm: 96.50 vali_Word: 73.82, vali_Topo: 96.06, vali_assm: 90.74\n",
      "[37400] , kl_loss 51.53, Word: 89.01, Topo: 99.11, Assm: 96.88 vali_Word: 73.10, vali_Topo: 95.96, vali_assm: 89.63\n",
      "epoch :  114\n",
      "[37600] , kl_loss 50.85, Word: 88.71, Topo: 99.11, Assm: 96.81 vali_Word: 73.06, vali_Topo: 96.00, vali_assm: 90.09\n",
      "[37800] , kl_loss 51.50, Word: 88.55, Topo: 99.02, Assm: 96.73 vali_Word: 73.38, vali_Topo: 96.06, vali_assm: 89.80\n",
      "epoch :  115\n",
      "[38000] , kl_loss 51.61, Word: 89.16, Topo: 99.17, Assm: 96.96 vali_Word: 74.00, vali_Topo: 96.08, vali_assm: 90.26\n",
      "epoch :  116\n",
      "[38200] , kl_loss 51.23, Word: 89.06, Topo: 99.07, Assm: 96.64 vali_Word: 73.28, vali_Topo: 96.11, vali_assm: 90.44\n",
      "[38400] , kl_loss 51.23, Word: 88.96, Topo: 99.14, Assm: 97.01 vali_Word: 74.10, vali_Topo: 96.00, vali_assm: 90.20\n",
      "epoch :  117\n",
      "[38600] , kl_loss 51.53, Word: 88.91, Topo: 99.08, Assm: 96.82 vali_Word: 73.66, vali_Topo: 96.06, vali_assm: 90.57\n",
      "[38800] , kl_loss 51.02, Word: 88.97, Topo: 99.10, Assm: 96.87 vali_Word: 73.37, vali_Topo: 95.82, vali_assm: 90.77\n",
      "epoch :  118\n",
      "[39000] , kl_loss 50.51, Word: 88.80, Topo: 99.06, Assm: 97.17 vali_Word: 74.05, vali_Topo: 96.16, vali_assm: 90.08\n",
      "epoch :  119\n",
      "[39200] , kl_loss 50.70, Word: 88.89, Topo: 99.09, Assm: 96.78 vali_Word: 75.04, vali_Topo: 96.08, vali_assm: 90.04\n",
      "[39400] , kl_loss 50.74, Word: 89.21, Topo: 99.15, Assm: 97.07 vali_Word: 73.38, vali_Topo: 96.01, vali_assm: 90.04\n",
      "epoch :  120\n",
      "[39600] , kl_loss 50.03, Word: 89.19, Topo: 99.13, Assm: 96.83 vali_Word: 73.91, vali_Topo: 96.07, vali_assm: 89.47\n",
      "[39800] , kl_loss 48.75, Word: 88.70, Topo: 99.00, Assm: 96.84 vali_Word: 74.49, vali_Topo: 96.05, vali_assm: 90.18\n",
      "epoch :  121\n",
      "[40000] , kl_loss 48.65, Word: 89.23, Topo: 99.01, Assm: 96.65 vali_Word: 73.80, vali_Topo: 96.03, vali_assm: 89.12\n",
      "epoch :  122\n",
      "[40200] , kl_loss 48.43, Word: 89.13, Topo: 99.08, Assm: 96.83 vali_Word: 73.59, vali_Topo: 96.02, vali_assm: 89.95\n",
      "[40400] , kl_loss 48.21, Word: 89.37, Topo: 99.08, Assm: 96.50 vali_Word: 74.11, vali_Topo: 95.87, vali_assm: 89.91\n",
      "epoch :  123\n",
      "[40600] , kl_loss 48.15, Word: 89.41, Topo: 99.17, Assm: 96.72 vali_Word: 74.50, vali_Topo: 96.07, vali_assm: 89.61\n",
      "epoch :  124\n",
      "[40800] , kl_loss 48.08, Word: 89.05, Topo: 99.14, Assm: 96.21 vali_Word: 73.50, vali_Topo: 95.93, vali_assm: 89.30\n",
      "[41000] , kl_loss 48.22, Word: 89.55, Topo: 99.25, Assm: 96.73 vali_Word: 73.22, vali_Topo: 96.15, vali_assm: 89.75\n",
      "epoch :  125\n",
      "[41200] , kl_loss 48.29, Word: 89.54, Topo: 99.15, Assm: 96.99 vali_Word: 74.16, vali_Topo: 96.00, vali_assm: 89.42\n",
      "[41400] , kl_loss 47.98, Word: 89.48, Topo: 99.14, Assm: 97.20 vali_Word: 74.02, vali_Topo: 95.82, vali_assm: 88.64\n",
      "epoch :  126\n",
      "[41600] , kl_loss 48.11, Word: 89.56, Topo: 99.09, Assm: 97.00 vali_Word: 73.58, vali_Topo: 96.02, vali_assm: 90.19\n",
      "epoch :  127\n",
      "[41800] , kl_loss 48.00, Word: 89.34, Topo: 99.17, Assm: 96.73 vali_Word: 73.31, vali_Topo: 95.91, vali_assm: 89.78\n",
      "[42000] , kl_loss 47.89, Word: 89.93, Topo: 99.24, Assm: 97.30 vali_Word: 74.70, vali_Topo: 96.18, vali_assm: 90.01\n",
      "epoch :  128\n",
      "[42200] , kl_loss 47.85, Word: 89.55, Topo: 99.11, Assm: 97.27 vali_Word: 73.52, vali_Topo: 96.11, vali_assm: 89.49\n",
      "[42400] , kl_loss 47.53, Word: 89.06, Topo: 99.13, Assm: 96.77 vali_Word: 73.66, vali_Topo: 96.11, vali_assm: 89.89\n",
      "epoch :  129\n",
      "[42600] , kl_loss 48.15, Word: 89.69, Topo: 99.17, Assm: 97.01 vali_Word: 74.43, vali_Topo: 96.08, vali_assm: 89.29\n",
      "epoch :  130\n",
      "[42800] , kl_loss 47.54, Word: 89.24, Topo: 99.13, Assm: 96.43 vali_Word: 73.99, vali_Topo: 96.10, vali_assm: 89.91\n",
      "[43000] , kl_loss 46.25, Word: 89.57, Topo: 99.18, Assm: 96.93 vali_Word: 74.11, vali_Topo: 96.02, vali_assm: 89.60\n",
      "epoch :  131\n",
      "[43200] , kl_loss 46.38, Word: 89.60, Topo: 99.15, Assm: 97.14 vali_Word: 73.75, vali_Topo: 95.80, vali_assm: 89.52\n",
      "[43400] , kl_loss 46.42, Word: 89.71, Topo: 99.13, Assm: 96.68 vali_Word: 74.34, vali_Topo: 95.83, vali_assm: 89.65\n",
      "epoch :  132\n",
      "[43600] , kl_loss 46.55, Word: 89.99, Topo: 99.15, Assm: 97.09 vali_Word: 74.52, vali_Topo: 95.88, vali_assm: 89.22\n",
      "epoch :  133\n",
      "[43800] , kl_loss 45.86, Word: 89.35, Topo: 99.12, Assm: 97.20 vali_Word: 74.42, vali_Topo: 96.05, vali_assm: 89.53\n",
      "[44000] , kl_loss 45.69, Word: 89.98, Topo: 99.21, Assm: 97.22 vali_Word: 73.74, vali_Topo: 96.01, vali_assm: 88.96\n",
      "epoch :  134\n",
      "[44200] , kl_loss 46.10, Word: 89.80, Topo: 99.16, Assm: 97.54 vali_Word: 73.80, vali_Topo: 96.09, vali_assm: 90.21\n",
      "[44400] , kl_loss 46.58, Word: 90.02, Topo: 99.20, Assm: 96.76 vali_Word: 73.62, vali_Topo: 96.01, vali_assm: 89.81\n",
      "epoch :  135\n",
      "[44600] , kl_loss 45.83, Word: 90.02, Topo: 99.26, Assm: 97.38 vali_Word: 72.51, vali_Topo: 96.15, vali_assm: 89.63\n",
      "epoch :  136\n",
      "[44800] , kl_loss 45.46, Word: 89.24, Topo: 99.07, Assm: 97.02 vali_Word: 74.47, vali_Topo: 95.86, vali_assm: 89.59\n",
      "[45000] , kl_loss 45.91, Word: 89.76, Topo: 99.24, Assm: 97.28 vali_Word: 74.14, vali_Topo: 96.03, vali_assm: 89.31\n",
      "epoch :  137\n",
      "[45200] , kl_loss 46.20, Word: 89.91, Topo: 99.09, Assm: 96.88 vali_Word: 74.66, vali_Topo: 96.09, vali_assm: 89.94\n",
      "[45400] , kl_loss 45.37, Word: 89.79, Topo: 99.11, Assm: 96.98 vali_Word: 73.30, vali_Topo: 96.17, vali_assm: 89.77\n",
      "epoch :  138\n",
      "[45600] , kl_loss 45.51, Word: 90.53, Topo: 99.21, Assm: 96.95 vali_Word: 73.89, vali_Topo: 96.06, vali_assm: 89.20\n",
      "epoch :  139\n",
      "[45800] , kl_loss 45.88, Word: 89.93, Topo: 99.14, Assm: 97.03 vali_Word: 74.65, vali_Topo: 96.00, vali_assm: 90.53\n",
      "[46000] , kl_loss 45.60, Word: 90.34, Topo: 99.24, Assm: 97.51 vali_Word: 73.77, vali_Topo: 95.90, vali_assm: 89.13\n",
      "epoch :  140\n",
      "[46200] , kl_loss 44.40, Word: 90.13, Topo: 99.18, Assm: 97.07 vali_Word: 74.77, vali_Topo: 96.15, vali_assm: 90.16\n",
      "epoch :  141\n",
      "[46400] , kl_loss 44.55, Word: 89.85, Topo: 99.20, Assm: 96.82 vali_Word: 73.70, vali_Topo: 96.04, vali_assm: 89.25\n",
      "[46600] , kl_loss 43.99, Word: 90.16, Topo: 99.20, Assm: 97.21 vali_Word: 73.35, vali_Topo: 95.98, vali_assm: 89.37\n",
      "epoch :  142\n",
      "[46800] , kl_loss 44.03, Word: 90.13, Topo: 99.19, Assm: 97.00 vali_Word: 73.81, vali_Topo: 95.81, vali_assm: 88.71\n",
      "[47000] , kl_loss 43.54, Word: 89.90, Topo: 99.21, Assm: 96.85 vali_Word: 73.43, vali_Topo: 95.82, vali_assm: 89.54\n",
      "epoch :  143\n",
      "[47200] , kl_loss 43.89, Word: 90.41, Topo: 99.18, Assm: 97.10 vali_Word: 74.11, vali_Topo: 96.07, vali_assm: 89.94\n",
      "epoch :  144\n",
      "[47400] , kl_loss 44.14, Word: 89.61, Topo: 99.13, Assm: 97.05 vali_Word: 74.69, vali_Topo: 95.83, vali_assm: 89.35\n",
      "[47600] , kl_loss 44.06, Word: 90.31, Topo: 99.17, Assm: 96.85 vali_Word: 74.38, vali_Topo: 95.98, vali_assm: 89.93\n",
      "epoch :  145\n",
      "[47800] , kl_loss 43.64, Word: 90.45, Topo: 99.17, Assm: 97.21 vali_Word: 75.28, vali_Topo: 96.06, vali_assm: 90.20\n",
      "[48000] , kl_loss 43.24, Word: 89.41, Topo: 99.13, Assm: 96.83 vali_Word: 74.52, vali_Topo: 96.03, vali_assm: 90.37\n",
      "epoch :  146\n",
      "[48200] , kl_loss 42.98, Word: 90.39, Topo: 99.19, Assm: 97.24 vali_Word: 74.01, vali_Topo: 95.93, vali_assm: 90.18\n",
      "epoch :  147\n",
      "[48400] , kl_loss 43.61, Word: 89.74, Topo: 99.13, Assm: 97.31 vali_Word: 73.83, vali_Topo: 95.96, vali_assm: 89.04\n",
      "[48600] , kl_loss 42.70, Word: 90.46, Topo: 99.25, Assm: 97.24 vali_Word: 74.21, vali_Topo: 95.93, vali_assm: 89.69\n",
      "epoch :  148\n",
      "[48800] , kl_loss 43.33, Word: 90.43, Topo: 99.27, Assm: 97.19 vali_Word: 74.23, vali_Topo: 95.79, vali_assm: 89.81\n",
      "[49000] , kl_loss 43.23, Word: 90.32, Topo: 99.19, Assm: 96.80 vali_Word: 73.70, vali_Topo: 96.08, vali_assm: 89.89\n",
      "epoch :  149\n",
      "[49200] , kl_loss 43.40, Word: 90.98, Topo: 99.32, Assm: 96.92 vali_Word: 74.56, vali_Topo: 95.92, vali_assm: 89.61\n",
      "epoch :  150\n",
      "[49400] , kl_loss 42.96, Word: 90.37, Topo: 99.20, Assm: 97.09 vali_Word: 73.96, vali_Topo: 95.72, vali_assm: 89.18\n",
      "[49600] , kl_loss 42.96, Word: 90.81, Topo: 99.23, Assm: 97.16 vali_Word: 73.77, vali_Topo: 96.02, vali_assm: 89.71\n",
      "epoch :  151\n",
      "[49800] , kl_loss 41.57, Word: 90.48, Topo: 99.22, Assm: 97.19 vali_Word: 73.88, vali_Topo: 95.76, vali_assm: 90.11\n",
      "[50000] , kl_loss 41.79, Word: 90.42, Topo: 99.20, Assm: 97.31 vali_Word: 73.93, vali_Topo: 96.10, vali_assm: 90.34\n",
      "epoch :  152\n",
      "[50200] , kl_loss 42.09, Word: 90.64, Topo: 99.28, Assm: 97.05 vali_Word: 73.18, vali_Topo: 95.85, vali_assm: 89.93\n",
      "epoch :  153\n",
      "[50400] , kl_loss 41.87, Word: 90.43, Topo: 99.23, Assm: 96.81 vali_Word: 74.34, vali_Topo: 95.95, vali_assm: 89.37\n",
      "[50600] , kl_loss 41.89, Word: 90.51, Topo: 99.21, Assm: 97.25 vali_Word: 74.13, vali_Topo: 95.83, vali_assm: 88.97\n",
      "epoch :  154\n",
      "[50800] , kl_loss 41.78, Word: 90.49, Topo: 99.21, Assm: 97.32 vali_Word: 74.19, vali_Topo: 95.83, vali_assm: 90.01\n",
      "epoch :  155\n",
      "[51000] , kl_loss 41.84, Word: 89.87, Topo: 99.17, Assm: 97.36 vali_Word: 74.23, vali_Topo: 95.96, vali_assm: 88.33\n",
      "[51200] , kl_loss 41.72, Word: 90.69, Topo: 99.22, Assm: 97.43 vali_Word: 74.33, vali_Topo: 95.85, vali_assm: 89.84\n",
      "epoch :  156\n",
      "[51400] , kl_loss 41.50, Word: 90.64, Topo: 99.28, Assm: 97.36 vali_Word: 74.01, vali_Topo: 96.12, vali_assm: 89.80\n",
      "[51600] , kl_loss 40.71, Word: 90.39, Topo: 99.19, Assm: 96.97 vali_Word: 74.48, vali_Topo: 95.85, vali_assm: 89.92\n",
      "epoch :  157\n",
      "[51800] , kl_loss 41.28, Word: 90.68, Topo: 99.24, Assm: 97.04 vali_Word: 74.28, vali_Topo: 95.88, vali_assm: 89.87\n",
      "epoch :  158\n",
      "[52000] , kl_loss 41.54, Word: 90.31, Topo: 99.21, Assm: 97.09 vali_Word: 74.16, vali_Topo: 95.90, vali_assm: 89.73\n",
      "[52200] , kl_loss 41.22, Word: 90.64, Topo: 99.31, Assm: 97.46 vali_Word: 74.86, vali_Topo: 96.05, vali_assm: 89.58\n",
      "epoch :  159\n",
      "[52400] , kl_loss 41.54, Word: 90.75, Topo: 99.20, Assm: 97.19 vali_Word: 74.39, vali_Topo: 95.87, vali_assm: 88.67\n",
      "[52600] , kl_loss 41.67, Word: 90.50, Topo: 99.24, Assm: 97.43 vali_Word: 74.56, vali_Topo: 96.09, vali_assm: 89.15\n",
      "epoch :  160\n",
      "[52800] , kl_loss 40.98, Word: 90.84, Topo: 99.24, Assm: 97.25 vali_Word: 73.89, vali_Topo: 95.81, vali_assm: 88.89\n",
      "epoch :  161\n",
      "[53000] , kl_loss 40.40, Word: 90.37, Topo: 99.17, Assm: 97.25 vali_Word: 74.84, vali_Topo: 95.87, vali_assm: 90.03\n",
      "[53200] , kl_loss 40.21, Word: 90.37, Topo: 99.21, Assm: 97.25 vali_Word: 73.49, vali_Topo: 96.11, vali_assm: 90.01\n",
      "epoch :  162\n",
      "[53400] , kl_loss 40.15, Word: 90.39, Topo: 99.23, Assm: 97.50 vali_Word: 74.71, vali_Topo: 95.86, vali_assm: 89.25\n",
      "[53600] , kl_loss 40.17, Word: 90.25, Topo: 99.17, Assm: 97.14 vali_Word: 74.08, vali_Topo: 95.89, vali_assm: 89.33\n",
      "epoch :  163\n",
      "[53800] , kl_loss 39.89, Word: 90.83, Topo: 99.28, Assm: 97.39 vali_Word: 74.76, vali_Topo: 95.99, vali_assm: 89.52\n",
      "epoch :  164\n",
      "[54000] , kl_loss 40.05, Word: 90.89, Topo: 99.24, Assm: 97.29 vali_Word: 73.83, vali_Topo: 95.92, vali_assm: 89.56\n",
      "[54200] , kl_loss 40.08, Word: 91.05, Topo: 99.29, Assm: 97.47 vali_Word: 74.24, vali_Topo: 96.13, vali_assm: 89.11\n",
      "epoch :  165\n",
      "[54400] , kl_loss 39.74, Word: 90.87, Topo: 99.20, Assm: 97.43 vali_Word: 74.20, vali_Topo: 96.16, vali_assm: 89.50\n",
      "[54600] , kl_loss 39.83, Word: 90.89, Topo: 99.26, Assm: 97.33 vali_Word: 74.39, vali_Topo: 96.10, vali_assm: 89.36\n",
      "epoch :  166\n",
      "[54800] , kl_loss 39.69, Word: 91.05, Topo: 99.32, Assm: 96.85 vali_Word: 74.48, vali_Topo: 95.92, vali_assm: 90.02\n",
      "epoch :  167\n",
      "[55000] , kl_loss 40.13, Word: 91.00, Topo: 99.26, Assm: 97.17 vali_Word: 73.49, vali_Topo: 96.07, vali_assm: 89.38\n",
      "[55200] , kl_loss 39.78, Word: 90.73, Topo: 99.14, Assm: 97.17 vali_Word: 73.56, vali_Topo: 96.09, vali_assm: 89.33\n",
      "epoch :  168\n",
      "[55400] , kl_loss 39.63, Word: 90.84, Topo: 99.28, Assm: 97.42 vali_Word: 73.54, vali_Topo: 95.98, vali_assm: 90.83\n",
      "[55600] , kl_loss 39.94, Word: 90.91, Topo: 99.25, Assm: 97.26 vali_Word: 73.87, vali_Topo: 95.93, vali_assm: 90.50\n",
      "epoch :  169\n",
      "[55800] , kl_loss 39.59, Word: 90.84, Topo: 99.25, Assm: 97.55 vali_Word: 74.60, vali_Topo: 96.06, vali_assm: 89.64\n",
      "epoch :  170\n",
      "[56000] , kl_loss 40.16, Word: 91.19, Topo: 99.32, Assm: 97.50 vali_Word: 75.14, vali_Topo: 95.89, vali_assm: 89.65\n",
      "[56200] , kl_loss 39.19, Word: 90.57, Topo: 99.21, Assm: 97.18 vali_Word: 74.53, vali_Topo: 96.03, vali_assm: 90.23\n",
      "epoch :  171\n",
      "[56400] , kl_loss 37.55, Word: 83.65, Topo: 98.32, Assm: 96.78 vali_Word: 73.21, vali_Topo: 95.75, vali_assm: 89.27\n",
      "epoch :  172\n",
      "[56600] , kl_loss 38.03, Word: 87.59, Topo: 98.92, Assm: 96.69 vali_Word: 74.43, vali_Topo: 95.96, vali_assm: 88.71\n",
      "[56800] , kl_loss 38.16, Word: 89.88, Topo: 99.15, Assm: 96.99 vali_Word: 74.56, vali_Topo: 95.61, vali_assm: 89.68\n",
      "epoch :  173\n",
      "[57000] , kl_loss 38.02, Word: 90.65, Topo: 99.22, Assm: 97.37 vali_Word: 74.02, vali_Topo: 96.05, vali_assm: 89.65\n",
      "[57200] , kl_loss 37.96, Word: 90.89, Topo: 99.34, Assm: 97.30 vali_Word: 73.62, vali_Topo: 96.03, vali_assm: 89.58\n",
      "epoch :  174\n",
      "[57400] , kl_loss 38.08, Word: 90.61, Topo: 99.27, Assm: 97.26 vali_Word: 72.85, vali_Topo: 95.71, vali_assm: 89.99\n",
      "epoch :  175\n",
      "[57600] , kl_loss 38.11, Word: 89.75, Topo: 99.18, Assm: 97.36 vali_Word: 73.85, vali_Topo: 95.98, vali_assm: 90.18\n",
      "[57800] , kl_loss 38.07, Word: 91.00, Topo: 99.29, Assm: 97.60 vali_Word: 74.08, vali_Topo: 95.98, vali_assm: 89.93\n",
      "epoch :  176\n",
      "[58000] , kl_loss 38.32, Word: 91.43, Topo: 99.29, Assm: 97.06 vali_Word: 74.69, vali_Topo: 95.98, vali_assm: 88.18\n",
      "[58200] , kl_loss 37.85, Word: 91.26, Topo: 99.26, Assm: 97.33 vali_Word: 74.13, vali_Topo: 96.03, vali_assm: 89.89\n",
      "epoch :  177\n",
      "[58400] , kl_loss 37.72, Word: 91.14, Topo: 99.34, Assm: 97.65 vali_Word: 73.87, vali_Topo: 96.08, vali_assm: 89.56\n",
      "epoch :  178\n",
      "[58600] , kl_loss 37.99, Word: 91.13, Topo: 99.22, Assm: 97.13 vali_Word: 74.22, vali_Topo: 96.02, vali_assm: 89.57\n",
      "[58800] , kl_loss 38.32, Word: 91.09, Topo: 99.29, Assm: 97.32 vali_Word: 74.32, vali_Topo: 95.96, vali_assm: 89.94\n",
      "epoch :  179\n",
      "[59000] , kl_loss 38.48, Word: 91.22, Topo: 99.27, Assm: 97.30 vali_Word: 74.72, vali_Topo: 95.87, vali_assm: 89.41\n",
      "[59200] , kl_loss 38.31, Word: 90.98, Topo: 99.27, Assm: 97.17 vali_Word: 73.48, vali_Topo: 96.06, vali_assm: 89.69\n",
      "epoch :  180\n",
      "[59400] , kl_loss 38.35, Word: 91.40, Topo: 99.31, Assm: 97.74 vali_Word: 74.43, vali_Topo: 96.00, vali_assm: 90.09\n",
      "epoch :  181\n",
      "[59600] , kl_loss 37.55, Word: 91.36, Topo: 99.24, Assm: 97.27 vali_Word: 74.45, vali_Topo: 96.00, vali_assm: 89.78\n",
      "[59800] , kl_loss 37.38, Word: 91.19, Topo: 99.22, Assm: 97.22 vali_Word: 74.81, vali_Topo: 95.79, vali_assm: 89.52\n",
      "epoch :  182\n",
      "[60000] , kl_loss 37.15, Word: 91.37, Topo: 99.28, Assm: 97.48 vali_Word: 74.99, vali_Topo: 95.92, vali_assm: 90.23\n",
      "[60200] , kl_loss 37.30, Word: 90.77, Topo: 99.24, Assm: 96.89 vali_Word: 74.47, vali_Topo: 95.90, vali_assm: 88.72\n",
      "epoch :  183\n",
      "[60400] , kl_loss 37.60, Word: 91.43, Topo: 99.35, Assm: 97.76 vali_Word: 73.69, vali_Topo: 95.97, vali_assm: 90.11\n",
      "epoch :  184\n",
      "[60600] , kl_loss 37.13, Word: 91.25, Topo: 99.25, Assm: 97.22 vali_Word: 74.61, vali_Topo: 95.83, vali_assm: 90.18\n",
      "[60800] , kl_loss 37.51, Word: 90.68, Topo: 99.17, Assm: 97.45 vali_Word: 73.79, vali_Topo: 96.02, vali_assm: 89.23\n",
      "epoch :  185\n",
      "[61000] , kl_loss 37.34, Word: 87.88, Topo: 98.96, Assm: 97.04 vali_Word: 73.16, vali_Topo: 95.87, vali_assm: 90.69\n",
      "epoch :  186\n",
      "[61200] , kl_loss 37.27, Word: 89.10, Topo: 99.13, Assm: 96.56 vali_Word: 74.46, vali_Topo: 95.88, vali_assm: 89.04\n",
      "[61400] , kl_loss 37.33, Word: 90.74, Topo: 99.20, Assm: 97.30 vali_Word: 74.53, vali_Topo: 95.80, vali_assm: 90.64\n",
      "epoch :  187\n",
      "[61600] , kl_loss 36.67, Word: 91.09, Topo: 99.30, Assm: 97.20 vali_Word: 74.88, vali_Topo: 96.06, vali_assm: 90.08\n",
      "[61800] , kl_loss 36.75, Word: 91.07, Topo: 99.27, Assm: 97.15 vali_Word: 74.16, vali_Topo: 95.89, vali_assm: 90.14\n",
      "epoch :  188\n",
      "[62000] , kl_loss 36.80, Word: 91.28, Topo: 99.32, Assm: 97.79 vali_Word: 74.53, vali_Topo: 96.03, vali_assm: 90.45\n",
      "epoch :  189\n",
      "[62200] , kl_loss 37.21, Word: 91.02, Topo: 99.22, Assm: 97.33 vali_Word: 75.41, vali_Topo: 96.23, vali_assm: 89.18\n",
      "[62400] , kl_loss 36.95, Word: 91.83, Topo: 99.36, Assm: 97.62 vali_Word: 74.33, vali_Topo: 95.85, vali_assm: 89.93\n",
      "epoch :  190\n",
      "[62600] , kl_loss 36.90, Word: 91.29, Topo: 99.26, Assm: 97.71 vali_Word: 73.50, vali_Topo: 95.85, vali_assm: 89.38\n",
      "[62800] , kl_loss 36.14, Word: 91.25, Topo: 99.36, Assm: 97.38 vali_Word: 74.57, vali_Topo: 95.87, vali_assm: 89.07\n",
      "epoch :  191\n",
      "[63000] , kl_loss 35.93, Word: 91.85, Topo: 99.35, Assm: 97.23 vali_Word: 73.57, vali_Topo: 95.87, vali_assm: 89.88\n",
      "epoch :  192\n",
      "[63200] , kl_loss 36.05, Word: 90.98, Topo: 99.32, Assm: 97.48 vali_Word: 74.31, vali_Topo: 95.99, vali_assm: 90.28\n",
      "[63400] , kl_loss 36.06, Word: 91.69, Topo: 99.32, Assm: 97.42 vali_Word: 74.09, vali_Topo: 95.81, vali_assm: 88.80\n",
      "epoch :  193\n",
      "[63600] , kl_loss 36.11, Word: 91.27, Topo: 99.28, Assm: 97.39 vali_Word: 74.17, vali_Topo: 95.86, vali_assm: 90.24\n",
      "[63800] , kl_loss 36.05, Word: 91.23, Topo: 99.25, Assm: 97.40 vali_Word: 74.33, vali_Topo: 95.98, vali_assm: 89.25\n",
      "epoch :  194\n",
      "[64000] , kl_loss 35.86, Word: 91.44, Topo: 99.22, Assm: 97.12 vali_Word: 72.02, vali_Topo: 95.46, vali_assm: 90.09\n",
      "epoch :  195\n",
      "[64200] , kl_loss 35.22, Word: 86.53, Topo: 98.79, Assm: 96.73 vali_Word: 74.71, vali_Topo: 95.84, vali_assm: 90.69\n",
      "[64400] , kl_loss 35.85, Word: 89.83, Topo: 99.13, Assm: 96.74 vali_Word: 74.53, vali_Topo: 95.96, vali_assm: 90.18\n",
      "epoch :  196\n",
      "[64600] , kl_loss 35.72, Word: 90.72, Topo: 99.29, Assm: 97.52 vali_Word: 75.18, vali_Topo: 95.86, vali_assm: 89.62\n",
      "[64800] , kl_loss 35.44, Word: 91.33, Topo: 99.29, Assm: 97.60 vali_Word: 74.98, vali_Topo: 96.14, vali_assm: 89.85\n",
      "epoch :  197\n",
      "[65000] , kl_loss 35.68, Word: 91.71, Topo: 99.41, Assm: 97.64 vali_Word: 74.30, vali_Topo: 96.00, vali_assm: 89.38\n",
      "epoch :  198\n",
      "[65200] , kl_loss 35.67, Word: 91.05, Topo: 99.34, Assm: 97.39 vali_Word: 73.75, vali_Topo: 96.08, vali_assm: 89.67\n",
      "[65400] , kl_loss 35.69, Word: 91.79, Topo: 99.25, Assm: 97.51 vali_Word: 74.35, vali_Topo: 95.96, vali_assm: 89.25\n",
      "epoch :  199\n",
      "[65600] , kl_loss 35.47, Word: 91.78, Topo: 99.32, Assm: 97.35 vali_Word: 74.51, vali_Topo: 95.88, vali_assm: 90.02\n",
      "[65800] , kl_loss 35.73, Word: 91.57, Topo: 99.30, Assm: 97.72 vali_Word: 73.91, vali_Topo: 95.90, vali_assm: 90.17\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from torch.autograd import Variable\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "pbar = None\n",
    "train_dataset.batch_size = 20\n",
    "vali_dataset.batch_size = 10\n",
    "\n",
    "anneal_iter = 7400\n",
    "\n",
    "word_rate=1.0\n",
    "topo_rate=1.0\n",
    "assm_rate=1.0\n",
    "\n",
    "beta = 0\n",
    "step_beta = 0.002\n",
    "kl_anneal_iter = 10 # epoch\n",
    "max_beta = 1\n",
    "warmup = 50 # epoch\n",
    "\n",
    "fine_tunning_warmup=30 # epoch\n",
    "\n",
    "def training(max_epoch = 100):\n",
    "    global pbar\n",
    "    global beta\n",
    "    total_step = 0\n",
    "    meters = np.zeros(7)\n",
    "    vali_meters = np.zeros(6)\n",
    "    with open(\"log2.csv\",\"w\") as f:\n",
    "        f.write(\"epoch,iter.,kl_loss,word,topo,assm,wors_loss,topo_loss,assm_loss,vali word,vali topo,vali assm,vali_word_loss,vali_topo_loss,vali_assm_loss\\n\")\n",
    "    for epoch in range(max_epoch):\n",
    "        if epoch % kl_anneal_iter == 0 and epoch >= warmup:\n",
    "            beta = min(max_beta, beta + step_beta)\n",
    "        print(\"epoch : \",epoch)\n",
    "        for batch in train_dataset:\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            total_step+=1\n",
    "            #pbar.update(1)\n",
    "            x = x.to('cuda')\n",
    "            y = y.to('cuda')\n",
    "            \n",
    "            enc_model.zero_grad()\n",
    "            dec_model.zero_grad()\n",
    "            enc_optimizer.zero_grad()\n",
    "            dec_optimizer.zero_grad()\n",
    "            \n",
    "            h,kl_loss = enc_model(x,y,training=True,sample=True)\n",
    "            tree_vec = h[:,:int(h.shape[1]/2)]\n",
    "            mol_vec  = h[:,int(h.shape[1]/2):]\n",
    "            _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "            word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "            assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "            total_loss = word_loss*word_rate+\\\n",
    "                         topo_loss*topo_rate+\\\n",
    "                         assm_loss*assm_rate+\\\n",
    "                         kl_loss*beta\n",
    "            total_loss.backward()\n",
    "            enc_optimizer.step()\n",
    "            if epoch >= fine_tunning_warmup:\n",
    "                dec_optimizer.step()\n",
    "            \n",
    "            del x,y,h\n",
    "\n",
    "            meters = meters + np.array([kl_loss.item(),word_acc * 100, topo_acc * 100, assm_acc * 100,word_loss.item(),topo_loss.item(),assm_loss.item()])\n",
    "            if total_step % 200 == 0:\n",
    "                vali_total = 0\n",
    "                for batch in vali_dataset:\n",
    "                    x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "                    x = x.to('cuda')\n",
    "                    y = y.to('cuda')\n",
    "                    with torch.no_grad():\n",
    "                        h,_ = enc_model(x,y,training=False,sample=False)\n",
    "                        tree_vec = h[:,:int(h.shape[1]/2)]\n",
    "                        mol_vec  = h[:,int(h.shape[1]/2):]\n",
    "                        _, x_tree_mess = dec_model.jtnn(*x_jtenc_holder)\n",
    "                        word_loss, topo_loss, word_acc, topo_acc = dec_model.decoder(x_batch,tree_vec)\n",
    "                        assm_loss, assm_acc = dec_model.assm(x_batch, x_jtmpn_holder, mol_vec , x_tree_mess)\n",
    "                        vali_meters = vali_meters + np.array([word_acc * 100, topo_acc * 100, assm_acc * 100,word_loss.item(),topo_loss.item(),assm_loss.item()])\n",
    "                        vali_total += 1    \n",
    "                    del x,y,h\n",
    "                    \n",
    "                meters /= 200\n",
    "                vali_meters /= vali_total\n",
    "                print (\"[%d] , kl_loss %.2f, Word: %.2f, Topo: %.2f, Assm: %.2f vali_Word: %.2f, vali_Topo: %.2f, vali_assm: %.2f\" % \\\n",
    "                    (total_step, meters[0], meters[1], meters[2],meters[3], vali_meters[0],vali_meters[1],vali_meters[2])   )             \n",
    "                with open(\"log2.csv\",\"a\") as f:\n",
    "                    f.write(\"%d,%d,\" % (epoch,total_step))\n",
    "                    f.write(\"%.2f,%.2f,%.2f,%.2f,%.2f,%.2f,%.2f,\" % (meters[0], meters[1], meters[2],meters[3],meters[4],meters[5],meters[6]))\n",
    "                    f.write(\"%.2f,%.2f,%.2f,%.2f,%.2f,%.2f\\n\" % (vali_meters[0],vali_meters[1],vali_meters[2],vali_meters[3],vali_meters[4],vali_meters[5]))\n",
    "                sys.stdout.flush()\n",
    "                meters *= 0\n",
    "                vali_meters *= 0\n",
    "            if total_step % 200 == 0:\n",
    "                torch.save(enc_model.state_dict(), \"./enc_model\" + \"/model.iter-\" + str(total_step))\n",
    "                torch.save(dec_model.state_dict(), \"./dec_model\" + \"/model.iter-\" + str(total_step))\n",
    "            #if total_step % anneal_iter == 0:\n",
    "                #scheduler.step()\n",
    "\n",
    "#import pdb; pdb.set_trace()\n",
    "try:\n",
    "    #if pbar is None:\n",
    "        #pbar = tqdm()\n",
    "    if not os.path.exists(\"./enc_model\"):\n",
    "        os.mkdir(\"./enc_model\")\n",
    "    if not os.path.exists(\"./dec_model\"):\n",
    "        os.mkdir(\"./dec_model\")\n",
    "    training(200)\n",
    "except RuntimeError as e:\n",
    "    #if pbar is not None:\n",
    "        #del pbar\n",
    "    import traceback\n",
    "    print(traceback.format_exc())\n",
    "    #import pdb; pdb.set_trace()\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vali_data.pkl 732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "73it [1:25:09, 50.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./enc_model/model.iter-65800\"\n",
    "if model_path is not None:\n",
    "    enc_model.load_state_dict(torch.load(model_path,map_location='cuda'))\n",
    "\n",
    "model_path = \"./dec_model/model.iter-65800\"\n",
    "if model_path is not None:\n",
    "    dec_model.load_state_dict(torch.load(model_path,map_location='cuda'))\n",
    "\n",
    "VOCAB_FILE = \"./MS_vocab.txt\"\n",
    "\n",
    "vocab = [x.strip(\"\\r\\n \") for x in open(VOCAB_FILE,\"r\")]\n",
    "vocab = Vocab(vocab)\n",
    "\n",
    "vali_dataset_path = \"vali_data.pkl\"\n",
    "with open(vali_dataset_path,\"rb\") as f:\n",
    "    vali_dataset = pickle.load(f)\n",
    "print(vali_dataset_path,len(vali_dataset))\n",
    "    \n",
    "vali_dataset.batch_size = 10\n",
    "device = 'cuda'\n",
    "\n",
    "sample_rate_list = [\n",
    "    [0.0,1],\n",
    "    [1.0,5],\n",
    "    [3.0,10]\n",
    "]\n",
    "\n",
    "def evaluation():\n",
    "    ret = []\n",
    "    with torch.no_grad():\n",
    "        for batch_number,batch in tqdm(enumerate(vali_dataset)):\n",
    "            x_batch, x_jtenc_holder, x_mpn_holder, x_jtmpn_holder,x,y = batch\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            result_one_data=[]\n",
    "            for x_data in x_batch:\n",
    "                result_one_data.append([Chem.MolToSmiles(Chem.MolFromSmiles(x_data.smiles),True)])\n",
    "            for sample_rate,times in sample_rate_list:\n",
    "                for No in range(times):\n",
    "                    h,_ = enc_model(x,y,training=False,sample=True,sample_rate=sample_rate)\n",
    "                    tree_vec = h[:,:int(h.shape[1]/2)]\n",
    "                    mol_vec  = h[:,int(h.shape[1]/2):]\n",
    "                    for num in range(h.size()[0]):\n",
    "                        predict_smiles = dec_model.decode(tree_vec[num].view(1,int(latent_size/2)),mol_vec[num].view(1,int(latent_size/2)),False)\n",
    "                \n",
    "                        #smilesの正規化\n",
    "                        predict_smiles = Chem.MolToSmiles(Chem.MolFromSmiles(predict_smiles),True)\n",
    "                \n",
    "                        result_one_data[num].append(predict_smiles)\n",
    "            ret.append(result_one_data)\n",
    "    return ret\n",
    "result = evaluation()\n",
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "730"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ret = []\n",
    "for one in result:\n",
    "    ret.extend(one)\n",
    "len(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def get_timef():\n",
    "    now = datetime.now()\n",
    "    return now.strftime(\"%Y%m%d-%H%M\")\n",
    "\n",
    "def _re_smiles(smiles1,smiles2):\n",
    "    #print(smiles1,smiles2)\n",
    "    smiles1 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles1),True)\n",
    "    smiles2 = Chem.MolToSmiles(Chem.MolFromSmiles(smiles2),True)\n",
    "    return smiles1 == smiles2\n",
    "\n",
    "def is_structural_isomer(smiles1,smiles2):\n",
    "    def Molecular_formula(smiles):\n",
    "        atoms = {}\n",
    "        mol = Chem.AddHs(Chem.MolFromSmiles(smiles))\n",
    "        for atom in mol.GetAtoms():\n",
    "            if not atom.GetSymbol() in atoms:\n",
    "                atoms[atom.GetSymbol()] = 1\n",
    "            else:\n",
    "                atoms[atom.GetSymbol()] += 1\n",
    "        return atoms\n",
    "    atoms1 = Molecular_formula(smiles1)\n",
    "    atoms2 = Molecular_formula(smiles2)\n",
    "    return atoms1 == atoms2\n",
    "\n",
    "def logout(one_sam,f):\n",
    "    global sample_rate_list\n",
    "    true_smiles = one_sam[0]\n",
    "    predict_smiles = one_sam[1:]\n",
    "    temp = []\n",
    "    cursor = 1\n",
    "    for one in one_sam:\n",
    "        f.write(one+\",\")\n",
    "    for one in predict_smiles:\n",
    "        f.write(str(_re_smiles(true_smiles,one))+\",\"+str(is_structural_isomer(true_smiles,one))+\",\")\n",
    "    ecfp_scores = []\n",
    "    maccs_scores = []\n",
    "    for one in predict_smiles:\n",
    "        true_mol = Chem.MolFromSmiles(true_smiles)\n",
    "        predict_mol = Chem.MolFromSmiles(one)\n",
    "        \n",
    "        true_fingerprint = AllChem.GetMorganFingerprint(true_mol,2)\n",
    "        predict_fingerprint = AllChem.GetMorganFingerprint(predict_mol,2)\n",
    "        ECFP_score = DataStructs.TanimotoSimilarity(true_fingerprint,predict_fingerprint)\n",
    "        ecfp_scores.append(ECFP_score)\n",
    "        \n",
    "        true_fingerprint = AllChem.GetMACCSKeysFingerprint(true_mol)\n",
    "        predict_fingerprint = AllChem.GetMACCSKeysFingerprint(predict_mol)\n",
    "        MACCS_score = DataStructs.TanimotoSimilarity(true_fingerprint,predict_fingerprint)\n",
    "        maccs_scores.append(MACCS_score)\n",
    "    for score in ecfp_scores:\n",
    "        f.write(str(score)+\",\")\n",
    "    f.write(\",\")\n",
    "    for score in maccs_scores:\n",
    "        f.write(str(score)+\",\")\n",
    "    f.write(\",\")\n",
    "        \n",
    "        \n",
    "    for rate,num in sample_rate_list:\n",
    "        temp.append(one_sam[cursor:cursor+num])\n",
    "        cursor += num\n",
    "    \n",
    "    return ecfp_scores,maccs_scores\n",
    "    #print(temp)\n",
    "\n",
    "ecfp_scores = []\n",
    "maccs_scores = []\n",
    "with open(\"area_sample.csv\",\"w\") as f:\n",
    "    f.write(\"{},,,,,,,,,,,,,,,,,,,,,,,,,,,,,\\n\".format(get_timef()))\n",
    "    f.write(\"total sample,{}\\n\".format(len(ret)))\n",
    "    f.write(\"sample_rate,number of sample\\n\")\n",
    "    for rate,num in sample_rate_list:\n",
    "        f.write(\"{},{}\\n\".format(rate,num))\n",
    "    for one in ret:\n",
    "        ecfp,maccs = logout(one,f)\n",
    "        ecfp_scores.append(ecfp)\n",
    "        maccs_scores.append(maccs)\n",
    "        f.write(\"\\n\")\n",
    "        pass\n",
    "    #logout(ret[0],f)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of score over 0.85: 35.75%\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "max_maccs_scores = np.asarray([np.max(score) for score in maccs_scores])\n",
    "frequency,ranges = np.histogram(max_maccs_scores,bins=20,range=(0.0,1.0))\n",
    "\n",
    "plt.hist(max_maccs_scores,bins=20,range=(0.0,1.0),rwidth=0.7)\n",
    "plt.vlines(0.85,0,np.max(frequency),'red')\n",
    "plt.title(\"Frequency of MACCS score\")\n",
    "plt.xlim(0,1.0)\n",
    "plt.xlabel('MACCS score [-]')\n",
    "plt.ylabel('Frequency [-]')\n",
    "plt.xticks([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.show()\n",
    "\n",
    "print(\"Percentage of score above 0.85: {:4.2f}%\".format(np.average(max_maccs_scores > 0.85)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
