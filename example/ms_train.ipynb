{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of model for identification of molecular structure from mass spectrum\n",
    "This notebook demonstrates the training and identification of our models.\n",
    "Before running this notebook, you need to run DataFetch.ipynb Preprocess.ipynb train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [04:41:42] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JTVAE]\n",
      "hidden_size = 100\n",
      "latent_size = 56\n",
      "depthT = 20\n",
      "depthG = 5\n",
      "\n",
      "[PEAK_ENCODER]\n",
      "max_mpz = 1000\n",
      "embedding_size = 10\n",
      "conv1_channel = 64\n",
      "kernel1_width = 5\n",
      "hidden_size = 200\n",
      "num_rnn_layers = 2\n",
      "bidirectional = False\n",
      "output_size = 56\n",
      "dropout_rate = 0.2\n",
      "use_batchnorm=False\n",
      "\n",
      "[TRAINING]\n",
      "; Number of epoch to finish learning\n",
      "max_epoch = 300\n",
      "; Number of epoch to start learning the decoder\n",
      "fine_tunning_warmup = 30\n",
      "\n",
      ";*********************************************;\n",
      ";  weight parameters\n",
      ";  1. word                = word_rate\n",
      ";  2. topology            = topo_rate\n",
      ";  3. assemble            = assm_rate\n",
      ";  4. KLDivergence        = beta\n",
      ";  5. L2 regularization   = reg_rate\n",
      ";\n",
      ";  total loss =\n",
      ";       word_rate * word_loss + \n",
      ";       topo_rate * topo_loss + \n",
      ";       assm_rate * assm_loss +\n",
      ";       beta * kl_loss +\n",
      ";       reg_rate * l2_reg\n",
      ";*********************************************;\n",
      "; weight of Word loss\n",
      "word_rate = 1\n",
      "; weight of topology loss\n",
      "topo_rate = 1\n",
      "; weight of assemble loss\n",
      "assm_rate = 1\n",
      "; weight of L2 regularization loss\n",
      "reg_rate = 0.0\n",
      "\n",
      "; Number of epoch to start appending step_beta to beta\n",
      "warmup = 100\n",
      "; Number of epoch \n",
      "kl_anneal_iter = 10\n",
      "; Initial value of beta\n",
      "init_beta = 0\n",
      "; append value of beta\n",
      "step_beta = 0.002\n",
      "; max value of beta\n",
      "max_beta = 1.0\n",
      "\n",
      "\n",
      "anneal_rate = 1.0\n",
      "anneal_iter = 10000\n",
      "\n",
      "; interval of iteration to validation calculation.\n",
      ";valid_interval = 1645 1 epoch = 329 iterations \n",
      "valid_interval = 3290\n",
      "; interval of iteration to save model parameters\n",
      "save_interval = 3290\n",
      "\n",
      "\n",
      "\n",
      "/tmp/ms-trainpbsxqboc/MS_vocab.txt\n",
      "/tmp/ms-trainpbsxqboc/massbank.pkl\n",
      "number of train dataset:  6580\n",
      "number of validation dataset:  720\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/workspace/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=1057, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=135, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=139, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n",
      "ms_peak_encoder_cnn(\n",
      "  (embedding): Embedding(1000, 10)\n",
      "  (convSequential): Sequential(\n",
      "    (conv1-1): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(11, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (conv1-2): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (conv1-3): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (transpose1): Transpose()\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(64, 200, num_layers=2, batch_first=True)\n",
      "    (gru-output): GRUOutput()\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (sampling): Sampling(\n",
      "    (var): Linear(in_features=200, out_features=56, bias=True)\n",
      "    (mean): Linear(in_features=200, out_features=56, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=56, out_features=56, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3979ea3eeba495f96f4836617ac4eb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=98700.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 epoch[3290] kl_loss 285.14, Word: 45.24, Topo: 84.71, Assm: 79.10 vali_Word: 52.06, vali_Topo: 86.40, vali_assm: 82.41 \n",
      "19 epoch[6580] kl_loss 285.93, Word: 53.20, Topo: 87.04, Assm: 80.96 vali_Word: 55.34, vali_Topo: 87.43, vali_assm: 84.08 \n",
      "29 epoch[9870] kl_loss 305.24, Word: 57.66, Topo: 88.53, Assm: 81.81 vali_Word: 55.76, vali_Topo: 88.01, vali_assm: 83.70 \n",
      "39 epoch[13160] kl_loss 253.51, Word: 73.43, Topo: 95.65, Assm: 91.19 vali_Word: 71.20, vali_Topo: 94.93, vali_assm: 89.47 \n",
      "49 epoch[16450] kl_loss 233.89, Word: 78.51, Topo: 97.41, Assm: 94.29 vali_Word: 72.28, vali_Topo: 95.29, vali_assm: 88.45 \n",
      "59 epoch[19740] kl_loss 238.48, Word: 81.83, Topo: 98.11, Assm: 95.15 vali_Word: 73.12, vali_Topo: 95.45, vali_assm: 89.58 \n",
      "69 epoch[23030] kl_loss 240.36, Word: 84.49, Topo: 98.55, Assm: 95.98 vali_Word: 72.63, vali_Topo: 95.72, vali_assm: 90.32 \n",
      "79 epoch[26320] kl_loss 243.98, Word: 86.47, Topo: 98.79, Assm: 96.40 vali_Word: 72.22, vali_Topo: 95.66, vali_assm: 89.27 \n",
      "89 epoch[29610] kl_loss 251.31, Word: 88.00, Topo: 98.96, Assm: 96.66 vali_Word: 72.16, vali_Topo: 95.66, vali_assm: 90.09 \n",
      "99 epoch[32900] kl_loss 252.29, Word: 89.17, Topo: 99.06, Assm: 96.99 vali_Word: 72.46, vali_Topo: 95.74, vali_assm: 90.43 \n",
      "109 epoch[36190] kl_loss 163.16, Word: 90.06, Topo: 99.13, Assm: 97.03 vali_Word: 74.00, vali_Topo: 95.74, vali_assm: 90.76 \n",
      "119 epoch[39480] kl_loss 114.20, Word: 90.71, Topo: 99.20, Assm: 96.91 vali_Word: 71.97, vali_Topo: 95.44, vali_assm: 89.50 \n",
      "129 epoch[42770] kl_loss 94.44, Word: 91.36, Topo: 99.24, Assm: 96.94 vali_Word: 72.81, vali_Topo: 95.66, vali_assm: 90.30 \n",
      "139 epoch[46060] kl_loss 82.58, Word: 91.72, Topo: 99.28, Assm: 96.90 vali_Word: 73.50, vali_Topo: 95.56, vali_assm: 90.05 \n",
      "149 epoch[49350] kl_loss 74.39, Word: 92.13, Topo: 99.32, Assm: 96.85 vali_Word: 72.42, vali_Topo: 95.67, vali_assm: 89.98 \n",
      "159 epoch[52640] kl_loss 68.83, Word: 92.21, Topo: 99.31, Assm: 96.87 vali_Word: 72.69, vali_Topo: 95.63, vali_assm: 89.95 \n",
      "169 epoch[55930] kl_loss 63.71, Word: 92.36, Topo: 99.32, Assm: 96.78 vali_Word: 73.29, vali_Topo: 95.69, vali_assm: 91.08 \n",
      "179 epoch[59220] kl_loss 59.79, Word: 92.75, Topo: 99.35, Assm: 96.89 vali_Word: 71.75, vali_Topo: 95.57, vali_assm: 90.33 \n",
      "189 epoch[62510] kl_loss 56.49, Word: 92.92, Topo: 99.37, Assm: 96.83 vali_Word: 72.34, vali_Topo: 95.70, vali_assm: 90.22 \n",
      "199 epoch[65800] kl_loss 53.66, Word: 93.03, Topo: 99.37, Assm: 96.93 vali_Word: 73.48, vali_Topo: 95.68, vali_assm: 90.04 \n",
      "209 epoch[69090] kl_loss 51.06, Word: 93.20, Topo: 99.39, Assm: 96.97 vali_Word: 72.89, vali_Topo: 95.47, vali_assm: 90.06 \n",
      "219 epoch[72380] kl_loss 49.29, Word: 93.16, Topo: 99.40, Assm: 96.99 vali_Word: 73.71, vali_Topo: 95.69, vali_assm: 89.43 \n",
      "229 epoch[75670] kl_loss 47.33, Word: 93.41, Topo: 99.41, Assm: 97.02 vali_Word: 72.56, vali_Topo: 95.56, vali_assm: 89.54 \n",
      "239 epoch[78960] kl_loss 45.90, Word: 93.39, Topo: 99.41, Assm: 97.06 vali_Word: 73.50, vali_Topo: 95.47, vali_assm: 89.90 \n",
      "249 epoch[82250] kl_loss 44.18, Word: 93.62, Topo: 99.42, Assm: 97.05 vali_Word: 73.56, vali_Topo: 95.37, vali_assm: 91.16 \n",
      "259 epoch[85540] kl_loss 42.76, Word: 93.54, Topo: 99.40, Assm: 97.09 vali_Word: 73.46, vali_Topo: 95.45, vali_assm: 89.72 \n",
      "269 epoch[88830] kl_loss 41.25, Word: 93.67, Topo: 99.44, Assm: 97.09 vali_Word: 73.25, vali_Topo: 95.53, vali_assm: 90.34 \n",
      "279 epoch[92120] kl_loss 40.04, Word: 93.67, Topo: 99.43, Assm: 97.19 vali_Word: 73.43, vali_Topo: 95.40, vali_assm: 90.38 \n",
      "289 epoch[95410] kl_loss 38.84, Word: 93.75, Topo: 99.42, Assm: 97.20 vali_Word: 73.36, vali_Topo: 95.74, vali_assm: 90.21 \n",
      "299 epoch[98700] kl_loss 37.76, Word: 93.80, Topo: 99.43, Assm: 97.26 vali_Word: 73.72, vali_Topo: 95.54, vali_assm: 91.10 \n",
      "\n",
      "Save last model.\n"
     ]
    }
   ],
   "source": [
    "from pcemg.scripts.ms_train import ms_train\n",
    "!cat model_config.ini\n",
    "ms_train(\"./MS_vocab.txt\",\"./massbank.pkl\",\"./vae_model/model.iter-160000\",'model_config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [01:01:32] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[JTVAE]\n",
      "hidden_size = 100\n",
      "latent_size = 56\n",
      "depthT = 20\n",
      "depthG = 5\n",
      "\n",
      "[PEAK_ENCODER]\n",
      "max_mpz = 1000\n",
      "embedding_size = 10\n",
      "conv1_channel = 64\n",
      "kernel1_width = 5\n",
      "hidden_size = 200\n",
      "num_rnn_layers = 2\n",
      "bidirectional = False\n",
      "output_size = 56\n",
      "dropout_rate = 0.2\n",
      "use_batchnorm=False\n",
      "\n",
      "[TRAINING]\n",
      "; Number of epoch to finish learning\n",
      "max_epoch = 300\n",
      "; Number of epoch to start learning the decoder\n",
      "fine_tunning_warmup = 30\n",
      "\n",
      ";*********************************************;\n",
      ";  weight parameters\n",
      ";  1. word                = word_rate\n",
      ";  2. topology            = topo_rate\n",
      ";  3. assemble            = assm_rate\n",
      ";  4. KLDivergence        = beta\n",
      ";  5. L2 regularization   = reg_rate\n",
      ";\n",
      ";  total loss =\n",
      ";       word_rate * word_loss + \n",
      ";       topo_rate * topo_loss + \n",
      ";       assm_rate * assm_loss +\n",
      ";       beta * kl_loss +\n",
      ";       reg_rate * l2_reg\n",
      ";*********************************************;\n",
      "; weight of Word loss\n",
      "word_rate = 1\n",
      "; weight of topology loss\n",
      "topo_rate = 1\n",
      "; weight of assemble loss\n",
      "assm_rate = 1\n",
      "; weight of L2 regularization loss\n",
      "reg_rate = 0.0\n",
      "\n",
      "; Number of epoch to start appending step_beta to beta\n",
      "warmup = 100\n",
      "; Number of epoch \n",
      "kl_anneal_iter = 10\n",
      "; Initial value of beta\n",
      "init_beta = 0\n",
      "; append value of beta\n",
      "step_beta = 0.002\n",
      "; max value of beta\n",
      "max_beta = 1.0\n",
      "\n",
      "\n",
      "anneal_rate = 1.0\n",
      "anneal_iter = 10000\n",
      "\n",
      "; interval of iteration to validation calculation.\n",
      ";valid_interval = 1645 1 epoch = 329 iterations \n",
      "valid_interval = 3290\n",
      "; interval of iteration to save model parameters\n",
      "save_interval = 3290\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/workspace/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Structuer of Encoder\n",
      " ms_peak_encoder_cnn(\n",
      "  (embedding): Embedding(1000, 10)\n",
      "  (convSequential): Sequential(\n",
      "    (conv1-1): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(11, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (conv1-2): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (conv1-3): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (transpose1): Transpose()\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(64, 200, num_layers=2, batch_first=True)\n",
      "    (gru-output): GRUOutput()\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (sampling): Sampling(\n",
      "    (var): Linear(in_features=200, out_features=56, bias=True)\n",
      "    (mean): Linear(in_features=200, out_features=56, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=56, out_features=56, bias=True)\n",
      ")\n",
      "Structuer of Decoder\n",
      " JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=1057, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=135, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=139, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "please choice iterate [3290 6580 ... 95410 98700] >>  last\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c565bac22043dabb74cb078360fb21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Encoding', max=36.0, style=ProgressStyle(description_widt…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bd38b03b9ab4baf91eb9e3027f9670c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=720.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "720\n",
      "17\n",
      "[0.35       0.64       0.83783784 0.8        1.         0.85\n",
      " 0.74285714 0.65517241 1.         0.93333333 0.58974359 0.53333333\n",
      " 0.625      0.52941176 0.54545455 0.96428571 0.83333333 0.94444444\n",
      " 0.65       1.         0.5        0.34782609 1.         0.64705882\n",
      " 0.85714286 0.58333333 0.84       0.72727273 0.51428571 0.8\n",
      " 0.75       1.         0.38461538 1.         0.68181818 1.\n",
      " 1.         0.33333333 0.86956522 0.21621622 0.46428571 0.25714286\n",
      " 0.48648649 0.27272727 0.4        0.77272727 1.         0.64516129\n",
      " 0.51515152 1.         1.         0.73684211 0.62162162 0.46428571\n",
      " 0.93333333 0.71428571 0.51428571 0.625      0.6        0.6\n",
      " 0.63157895 0.76470588 0.48148148 0.88235294 0.85185185 0.75862069\n",
      " 1.         0.68181818 0.91428571 0.48837209 0.78571429 1.\n",
      " 1.         0.54761905 1.         1.         0.875      0.6\n",
      " 0.92       0.91304348 0.47058824 1.         0.57692308 0.39285714\n",
      " 1.         0.5862069  0.81578947 0.57894737 1.         1.\n",
      " 0.8        0.3559322  1.         0.41176471 0.61290323 0.86666667\n",
      " 0.69565217 0.70588235 0.86666667 1.         0.78723404 0.63157895\n",
      " 0.58974359 1.         0.95       0.30769231 0.68181818 0.625\n",
      " 0.63333333 0.39473684 0.65217391 0.55       0.375      0.51724138\n",
      " 0.80952381 0.83333333 1.         0.22222222 0.57894737 0.32758621\n",
      " 0.55555556 0.76923077 0.5        0.33333333 0.67857143 0.53333333\n",
      " 0.89473684 0.21875    0.85714286 0.73684211 0.26470588 1.\n",
      " 0.94       0.51515152 0.72413793 1.         0.88461538 1.\n",
      " 0.77777778 0.28571429 0.61538462 0.53333333 0.6097561  1.\n",
      " 0.625      0.65       0.93333333 1.         0.60869565 0.88235294\n",
      " 1.         0.54285714 0.59090909 0.5625     0.61904762 0.76\n",
      " 0.82352941 1.         0.68965517 0.96153846 0.68181818 0.76086957\n",
      " 0.27777778 1.         0.43243243 1.         1.         0.55813953\n",
      " 0.64583333 0.66666667 0.73913043 0.60606061 0.61111111 0.63636364\n",
      " 0.65625    0.85185185 0.33333333 0.51612903 1.         0.66666667\n",
      " 0.89130435 0.74418605 0.82608696 0.45833333 0.47058824 1.\n",
      " 0.91891892 0.62068966 0.7804878  0.81818182 0.94736842 0.33333333\n",
      " 1.         0.79166667 0.45238095 0.375      1.         0.7027027\n",
      " 0.66666667 1.         1.         0.74358974 0.7826087  0.57692308\n",
      " 0.63157895 0.48275862 1.         0.57142857 0.66666667 0.59090909\n",
      " 0.375      1.         1.         0.65       1.         0.33333333\n",
      " 0.92307692 0.91304348 0.80769231 0.92857143 0.75555556 0.44444444\n",
      " 1.         0.55555556 1.         0.36842105 0.35       1.\n",
      " 1.         0.57777778 0.48484848 1.         0.63636364 0.53846154\n",
      " 0.46875    1.         0.4        0.30232558 0.69565217 0.64705882\n",
      " 1.         0.5        0.95       0.52       0.69230769 1.\n",
      " 0.875      1.         0.42857143 0.64       0.5        0.15789474\n",
      " 0.80952381 0.71428571 0.37931034 0.5483871  1.         0.52777778\n",
      " 0.63636364 0.86956522 1.         0.21212121 1.         0.40909091\n",
      " 0.65384615 0.80952381 0.51851852 0.42105263 1.         0.41666667\n",
      " 0.70588235 1.         0.41538462 0.90909091 0.27777778 0.27272727\n",
      " 0.68421053 0.86363636 0.5        0.48       0.89473684 1.\n",
      " 0.66666667 1.         0.84615385 0.63636364 0.74193548 1.\n",
      " 0.60869565 1.         1.         0.71052632 0.85185185 0.72727273\n",
      " 0.65       0.76       1.         1.         0.73913043 0.85714286\n",
      " 0.42424242 1.         0.92307692 0.15151515 1.         0.84210526\n",
      " 1.         1.         0.59259259 0.42857143 0.27272727 0.53488372\n",
      " 1.         0.6        1.         0.94117647 1.         0.71428571\n",
      " 0.68965517 0.90909091 0.5483871  1.         1.         0.53333333\n",
      " 0.72727273 0.63333333 1.         0.68421053 1.         1.\n",
      " 0.77777778 0.76923077 0.64       0.70833333 0.82926829 0.91891892\n",
      " 0.39130435 0.5        0.625      1.         1.         0.73333333\n",
      " 0.2        0.56666667 0.7        0.76       1.         0.58823529\n",
      " 1.         0.65       0.61111111 0.65       0.42857143 1.\n",
      " 0.72222222 0.51851852 1.         0.69444444 0.56521739 0.57777778\n",
      " 0.11111111 0.55555556 0.81818182 0.83870968 0.70833333 0.47619048\n",
      " 0.42857143 0.74193548 1.         1.         0.38709677 0.86956522\n",
      " 1.         1.         0.90909091 0.875      0.92307692 1.\n",
      " 0.54166667 0.81481481 0.91176471 0.65517241 0.95833333 0.84615385\n",
      " 0.52272727 0.77777778 0.57692308 1.         0.5        0.96969697\n",
      " 0.36842105 0.5        0.78947368 0.36       1.         0.61538462\n",
      " 0.95454545 0.27272727 0.77777778 0.53846154 0.58333333 0.41176471\n",
      " 0.78571429 0.26666667 1.         0.875      1.         0.96875\n",
      " 0.60465116 0.70833333 0.64       0.64285714 0.8        0.66666667\n",
      " 0.36666667 0.75       1.         0.38461538 0.71428571 0.80952381\n",
      " 0.83333333 0.9        1.         0.45833333 0.75       1.\n",
      " 1.         1.         0.5        0.65217391 1.         0.78571429\n",
      " 0.61538462 0.54545455 0.78947368 0.5        0.34042553 0.94444444\n",
      " 1.         0.83870968 1.         0.9        0.34615385 0.79166667\n",
      " 0.82608696 0.44642857 0.88       0.41666667 0.71875    1.\n",
      " 0.86206897 0.72222222 0.375      0.5483871  0.44       0.3125\n",
      " 0.72727273 0.69230769 1.         0.50877193 0.71428571 1.\n",
      " 0.86363636 0.77777778 0.63636364 1.         0.57692308 0.64\n",
      " 0.51428571 1.         0.80769231 0.88       0.73529412 1.\n",
      " 0.77777778 1.         1.         0.69230769 0.87096774 0.35\n",
      " 0.75       0.8        0.81818182 0.56       0.53571429 0.5\n",
      " 0.47619048 0.41935484 1.         0.7        0.5        0.46428571\n",
      " 1.         0.61538462 0.73333333 0.54761905 0.2962963  1.\n",
      " 1.         0.85714286 0.5        0.85714286 0.95       0.46428571\n",
      " 0.73684211 0.51724138 0.55555556 0.63333333 0.59259259 0.42307692\n",
      " 1.         0.45945946 0.28       0.53333333 0.85       0.55555556\n",
      " 0.62962963 0.88888889 1.         0.80645161 0.73076923 0.95\n",
      " 0.43902439 0.74193548 0.64705882 0.48148148 0.85714286 0.55882353\n",
      " 0.92307692 0.86842105 0.76923077 1.         0.95       0.86206897\n",
      " 0.96       0.54285714 0.9        0.64285714 1.         0.95454545\n",
      " 0.68421053 0.66666667 0.53846154 0.9        0.71428571 0.84615385\n",
      " 0.94736842 0.46511628 0.71428571 1.         1.         0.58974359\n",
      " 0.86666667 1.         1.         0.54545455 0.5        0.63333333\n",
      " 1.         0.5        0.625      1.         1.         0.56097561\n",
      " 0.88235294 1.         0.53731343 0.61538462 1.         0.63157895\n",
      " 1.         0.55555556 0.78571429 0.66666667 0.38095238 0.57894737\n",
      " 0.46       0.9        0.85714286 0.53333333 0.72727273 0.63043478\n",
      " 0.41860465 0.68421053 0.34146341 0.45       0.33333333 1.\n",
      " 0.70833333 0.88461538 0.39285714 1.         0.63333333 0.42105263\n",
      " 0.57894737 1.         0.2962963  0.875      0.5        0.625\n",
      " 0.88235294 0.83333333 1.         0.71428571 0.6        0.44117647\n",
      " 1.         1.         0.66666667 0.40677966 0.35       0.5625\n",
      " 0.375      1.         0.78947368 0.40909091 0.60714286 0.71428571\n",
      " 0.89655172 0.78571429 1.         0.52173913 0.69230769 1.\n",
      " 1.         0.70967742 1.         0.75       0.90909091 0.84848485\n",
      " 1.         0.74193548 0.44444444 0.81481481 1.         0.58823529\n",
      " 0.53846154 0.25       0.54545455 1.         0.45762712 0.34883721\n",
      " 0.58823529 0.57142857 0.73529412 0.55       0.75862069 0.82142857\n",
      " 0.44444444 0.6        1.         0.75       0.97435897 0.61111111\n",
      " 1.         1.         0.75       0.75862069 1.         0.42857143\n",
      " 1.         0.88461538 0.45       0.73076923 0.45454545 0.83333333\n",
      " 0.36       1.         0.65517241 1.         0.52173913 0.47619048\n",
      " 0.95454545 0.77272727 0.17241379 1.         0.65       0.85714286\n",
      " 0.65384615 0.55555556 1.         0.8        0.73076923 1.\n",
      " 0.47826087 0.66666667 0.88888889 0.58823529 0.53846154 0.70588235\n",
      " 1.         0.63157895 0.88888889 0.73913043 0.95121951 0.86666667\n",
      " 0.80952381 0.89473684 1.         0.47058824 0.5        0.92\n",
      " 0.95       0.70588235 0.66666667 0.38095238 0.88       0.44186047\n",
      " 0.63636364 1.         0.33962264 1.         0.875      0.75862069\n",
      " 0.35714286 0.82608696 1.         0.92307692 0.80769231 0.42105263]\n",
      "(720,)\n",
      "35.27777777777778\n"
     ]
    }
   ],
   "source": [
    "from pcemg.scripts.analysis_model import analysisModel\n",
    "analysisModel('./result-20200622-0441.tar.gz',eval_mode=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "RDKit WARNING: [08:54:22] Enabling RDKit 2019.09.3 jupyter extensions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[JTVAE]\n",
      "hidden_size = 100\n",
      "latent_size = 56\n",
      "depthT = 20\n",
      "depthG = 5\n",
      "\n",
      "[PEAK_ENCODER]\n",
      "max_mpz = 1000\n",
      "embedding_size = 10\n",
      "conv1_channel = 64\n",
      "kernel1_width = 5\n",
      "hidden_size = 200\n",
      "num_rnn_layers = 2\n",
      "bidirectional = False\n",
      "output_size = 56\n",
      "dropout_rate = 0.2\n",
      "use_batchnorm=False\n",
      "\n",
      "[TRAINING]\n",
      "; Number of epoch to finish learning\n",
      "max_epoch = 300\n",
      "; Number of epoch to start learning the decoder\n",
      "fine_tunning_warmup = 30\n",
      "\n",
      ";*********************************************;\n",
      ";  weight parameters\n",
      ";  1. word                = word_rate\n",
      ";  2. topology            = topo_rate\n",
      ";  3. assemble            = assm_rate\n",
      ";  4. KLDivergence        = beta\n",
      ";  5. L2 regularization   = reg_rate\n",
      ";\n",
      ";  total loss =\n",
      ";       word_rate * word_loss + \n",
      ";       topo_rate * topo_loss + \n",
      ";       assm_rate * assm_loss +\n",
      ";       beta * kl_loss +\n",
      ";       reg_rate * l2_reg\n",
      ";*********************************************;\n",
      "; weight of Word loss\n",
      "word_rate = 1\n",
      "; weight of topology loss\n",
      "topo_rate = 1\n",
      "; weight of assemble loss\n",
      "assm_rate = 1\n",
      "; weight of L2 regularization loss\n",
      "reg_rate = 0.0\n",
      "\n",
      "; Number of epoch to start appending step_beta to beta\n",
      "warmup = 100\n",
      "; Number of epoch \n",
      "kl_anneal_iter = 10\n",
      "; Initial value of beta\n",
      "init_beta = 0\n",
      "; append value of beta\n",
      "step_beta = 0.002\n",
      "; max value of beta\n",
      "max_beta = 1.0\n",
      "\n",
      "\n",
      "anneal_rate = 1.0\n",
      "anneal_iter = 10000\n",
      "\n",
      "; interval of iteration to validation calculation.\n",
      ";valid_interval = 1645 1 epoch = 329 iterations \n",
      "valid_interval = 3290\n",
      "; interval of iteration to save model parameters\n",
      "save_interval = 3290\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kusachi/anaconda3/envs/workspace/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Structuer of Encoder\n",
      " ms_peak_encoder_cnn(\n",
      "  (embedding): Embedding(1000, 10)\n",
      "  (convSequential): Sequential(\n",
      "    (conv1-1): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(11, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (conv1-2): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (conv1-3): conv_set(\n",
      "      (convSequential): Sequential(\n",
      "        (0): Conv1d(64, 64, kernel_size=(5,), stride=(1,), padding=(2,), padding_mode=zero)\n",
      "        (1): LeakyReLU(negative_slope=0.01)\n",
      "      )\n",
      "    )\n",
      "    (transpose1): Transpose()\n",
      "    (dropout1): Dropout(p=0.2, inplace=False)\n",
      "    (gru): GRU(64, 200, num_layers=2, batch_first=True)\n",
      "    (gru-output): GRUOutput()\n",
      "    (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (sampling): Sampling(\n",
      "    (var): Linear(in_features=200, out_features=56, bias=True)\n",
      "    (mean): Linear(in_features=200, out_features=56, bias=True)\n",
      "  )\n",
      "  (output): Linear(in_features=56, out_features=56, bias=True)\n",
      ")\n",
      "Structuer of Decoder\n",
      " JTNNVAE(\n",
      "  (jtnn): JTNNEncoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (outputNN): Sequential(\n",
      "      (0): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (1): ReLU()\n",
      "    )\n",
      "    (GRU): GraphGRU(\n",
      "      (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "      (W_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "      (U_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "      (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decoder): JTNNDecoder(\n",
      "    (embedding): Embedding(1057, 100)\n",
      "    (W_z): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (U_r): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_r): Linear(in_features=100, out_features=100, bias=True)\n",
      "    (W_h): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U): Linear(in_features=128, out_features=100, bias=True)\n",
      "    (U_i): Linear(in_features=200, out_features=100, bias=True)\n",
      "    (W_o): Linear(in_features=100, out_features=1057, bias=True)\n",
      "    (U_o): Linear(in_features=100, out_features=1, bias=True)\n",
      "    (pred_loss): CrossEntropyLoss()\n",
      "    (stop_loss): BCEWithLogitsLoss()\n",
      "  )\n",
      "  (jtmpn): JTMPN(\n",
      "    (W_i): Linear(in_features=40, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=135, out_features=100, bias=True)\n",
      "  )\n",
      "  (mpn): MPN(\n",
      "    (W_i): Linear(in_features=50, out_features=100, bias=False)\n",
      "    (W_h): Linear(in_features=100, out_features=100, bias=False)\n",
      "    (W_o): Linear(in_features=139, out_features=100, bias=True)\n",
      "  )\n",
      "  (A_assm): Linear(in_features=28, out_features=100, bias=False)\n",
      "  (assm_loss): CrossEntropyLoss()\n",
      "  (T_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (T_var): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_mean): Linear(in_features=100, out_features=28, bias=True)\n",
      "  (G_var): Linear(in_features=100, out_features=28, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "please choice iterate [3290 6580 ... 95410 98700] >>  last\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 730/730 [41:48<00:00,  3.44s/it]   \n"
     ]
    }
   ],
   "source": [
    "from pcemg.scripts.saliency_calc import saliencyCalc\n",
    "\n",
    "saliencyCalc(\"./result-20200622-0441.tar.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "workspace"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
